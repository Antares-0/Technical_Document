# 朴素贝叶斯算法

## 一、朴素贝叶斯算法简介
1. 朴素贝叶斯（naive Bayes）法是一种基于概率的机器学习算法。其核心原理在于利用贝叶斯定理计算给定数据样本下各类别的后验概率，并选择具有最高后验概率的类别作为该样本的预测类别
2. 朴素贝叶斯算法假设“特征工程非常完美，把所有特征都区分开了”，假设各个特征之间都是独立的，这也是它“朴素”的来源
3. 朴素贝叶斯算法的核心是贝叶斯定理：$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$
   - `P(Y|X)`：后验概率，给定特征X时类Y的概率
   - `P(X|Y)`：条件概率，类Y包含特征X的概率
   - `P(Y)`：先验概率，类Y的概率
   - `P(X)`：特征X的概率
   ```txt
   P(Y|X)：邮件包含“免费”时是垃圾邮件的概率。
   P(X|Y)：垃圾邮件中包含“免费”的概率。
   P(Y)：邮件是垃圾邮件的概率。
   P(X)：邮件包含“免费”的概率。
   
   现分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少？
   
   # X事件为选中了红球
   p(X) = 8/20
   # Y事件代表选中A容器
   p(Y) = 1/2
   # A容器中红球的概率
   p(X|Y) = 7/10
   # 选中一个红球，该球来自A容器的概率为
   p(Y|X) = p(X|Y) * p(Y)/p(X)
   p(Y|X) = 7/10 * 1/2 * 20/8 = 7/8
   ```
4. 朴素贝叶斯假设多有特征相互独立，大大简化了计算
   $$
   P(X_1,X_2,\dots,X_n | Y) = P(X_1 | Y) \cdot P(X_2 | Y) \cdot \dots \cdot P(X_n | Y) = \prod_{j=1}^n P(X_j | Y)
   $$
5. 极大似然估计：在朴素贝叶斯法中，学习意味着估计先验概率`P(Y)`和条件概率`P(X|Y)`。可以应用极大似然估计法估计相应的概率
   - 先验概率极大似然估计：$P(Y=C_k) = \frac{\sum_{i=1}^N I(y_i=C_k)}{N},\quad k=1,2,\dots,K$，`I`是示性函数，取值为0或者1
   - 条件概率的极大似然估计：$P(X_j=a_{jl} | Y=C_k) = \frac{\sum_{i=1}^N I(x_{ji}=a_{jl}, y_i=C_k)}{\sum_{i=1}^N I(y_i=C_k)}$
6. 贝叶斯估计：在朴素贝叶斯法中，学习意味着估计先验概率和条件概率。可以应用极大似然估计法估计相应的概率
   - 主要是为了解决在朴素贝叶斯算法中，某一个概率如果是零（比如没有对应特征的数据或其他情况），连乘会导致全部值都变成0
   - 先验概率的贝叶斯估计：添加了一个$\lambda$
   $$
   P_\lambda (Y=C_k) = \frac{\sum_{i=1}^N \left( I(y_i=C_k) + \lambda \right)}{N + K\lambda},\quad k=1,2,\dots,K
   $$
   - 条件概率的贝叶斯估计：也添加了一个$\lambda$
   $$
   P_\lambda (X_j=a_{jl} | Y=C_k) = \frac{\sum_{i=1}^N \left( I(x_{ji}=a_{jl}, y_i=C_k) + \lambda \right)}{\sum_{i=1}^N I(y_i=C_k) + L\lambda}
   $$
   - 式中$\lambda$≥0，当$\lambda$=0时就是极大似然估计，常取$\lambda$=1，这时称为拉普拉斯平滑

## 二、工作原理
1. 贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而朴素贝叶斯（Naive Bayes）分类是贝叶斯分类中最简单，也是常见的一种分类方法。朴素贝叶斯算法的核心思想是通过考虑特征概率来预测分类，即对于给出的待分类样本，求解在此样本出现的条件下各个类别出现的概率，哪个最大，就认为此待分类样本属于哪个类别。
2. 通过假设各个特征之间是独立的，分别使用贝叶斯公式计算后验概率，分别计算`P(包含免费时是垃圾邮件)`和`P(包含免费时不是垃圾邮件)`，概率最大的作为最终是不是垃圾邮件的判定

## <font color='yellow'>**三、关键点**</font>
1. 朴素贝叶斯算法种类：在scikit-learn中，包含3种朴素贝叶斯的分类算法
   - 先验概率为**高斯分布（正态分布）的朴素贝叶斯**：假设每个标签的数据都服从简单的正态分布
   - 先验概率为**多项式分布的朴素贝叶斯**：假设特征是由一个简单多项式分布生成的
   - 先验概率为**伯努利分布的朴素贝叶斯**：
2. 分布类型
   - 【离散型】伯努利分布：一次实验，只有成功和失败之分
     - 二分类标签（0/1）、一次预测是否正确
   - 【离散型】二项分布：n次独立的二分类实验，统计成功几次
     - 比如：n 次预测里对了几次、抛硬币 n 次正面几次
   - 【离散型】多项分布：每次有k个类别，做n次实验，统计每类出现几次
     - 比如：图像分类（猫 / 狗 / 鸟）、文本多标签计数
   - 【离散型】泊松分布：单位时间、空间里事件发生多少次
     - 比如：点击量、词频、异常次数、销量
   - 【离散型】卡方分布：正态分布的平方和，用于假设检验、拟合优度
     - 比如：特征独立性检验、分布是否匹配
   - 【离散型】贝塔分布：专门建模 “概率” 的分布（值域 0~1）
     - 比如：贝叶斯里二分类概率 p 的先验分布
   - 【离散型】狄利克雷分布：贝塔分布的多分类版，建模一组概率和为 1
     - 比如：LDA 主题模型、多分类概率的先验。
   - 【连续型】均匀分布：随机、公平、无偏好。
     - 比如：参数初始化、随机采样、Min-Max 归一化。
   - 【连续型】正态分布：自然界 & 误差最常见分布。
     - 比如：特征归一化、噪声建模、线性回归假设、高斯混合模型 GMM。
   - 【连续型】指数分布：时间间隔 / 寿命。
     - 比如：用户两次行为间隔、故障间隔、等待时间。
   - 【连续型】伽马分布：指数分布的推广，用来建模正实数、右偏数据。
     - 比如：等待时间总和、信号强度、生存分析。

## 四、优缺点
1. 优点：
   - 模型简单：基于简单的概率模型，易于理解和实现。
   - 分类速度快：主要进行概率计算，适用于大规模数据处理。
   - 对小规模数据友好：不需要大量数据来训练。
   - 适用于多分类：可以轻松处理多类别分类任务。
   - 对缺失数据不敏感：可以较好地处理含有缺失值的数据集。
2. 缺点：可能影响分类的准确性
   - 特征独立性假设：假设特征之间相互独立，这在现实中可能不成立，影响算法效果。
   - 对输入数据敏感：对数据的表达形式敏感，如连续型数据需要离散化，可能损失信息。
   - 对不平衡数据敏感：可能偏向于多数类，导致对少数类的预测准确率较低。
   - 先验概率计算：样本数量少时，先验概率计算可能不准确。
   - 不适合复杂关系数据：可能无法准确捕捉数据中的复杂关系。
3. 在文本分类、垃圾邮件过滤领域取得了显著的效果

## 五、实战

参考资料：
1. 尚硅谷机器学习视频：https://www.bilibili.com/video/BV1BYe4z5E9z
2. 朴素贝叶斯：https://blog.csdn.net/weixin_50804299/article/details/138348212
3. 朴素贝叶斯：https://blog.csdn.net/haha0332/article/details/112575122

