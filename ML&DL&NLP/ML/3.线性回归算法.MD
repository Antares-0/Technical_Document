# 线性回归算法

## 一、线性回归算法简介
1. 线性回归是一种用于建模两个或多个变量之间线性关系的统计方法。它通过拟合一条直线（或超平面）来描述自变量（输入特征）与因变量（输出目标）之间的关联，并可用于预测或分析变量间的影响关系。
2. 线性回归方程：`y = b0 + b1 * x1 + b2 * x2 + b3 * x3 + ...`
3. 线性回归应用场景举例
   - GDP预测：用历史数据（如投资、消费、出口）建立回归模型，预测GDP增长趋势
   - 广告效果评估：量化不同渠道广告投入对销售额的影响，优化预算分配
   - 药物剂量研究：分析药物剂量与患者生理指标（如血压、血糖）之间的关系
   - 产品质量控制：通过生产参数（温度、压力、原料配比）预测产品性能（如强度、耐久）
   - 政策效果评估：分析最低工资政策对就业率的影响，或环保法规对污染排放的抑制作用
   - 气候变化建模：用工业排放量、森林覆盖率等变量预测全球气温变化趋势
4. 本质是使用线性模型强行拟合各个特征，认为各个特征与最终结果呈现线性关系

## 二、工作原理
1. 损失函数：用于代表模型预测值与真实值之间的差距
   - 均方误差MSE：预测值与真实值之间的差值平方求和，相当于欧式距离、L2范数
   - 平均绝对误差MAE：预测值与真实值之间的差的绝对值求和，相当于L1范数
2. 持续优化参数，期望损失函数值最小

## <font color='yellow'>**三、关键点**</font>
1. 损失函数的选择：
   - <font color='yellow'>**均方误差MSE：最常用的损失函数（误差服从正态分布时最优）**</font>
     - 特点：
       - 均方误差对大误差比较敏感，因为平方项会放大较大的误差
       - 均方误差是凸函数，存在全局的唯一最小值，平方项又使损失函数处处可导，便于求解最优参数
       - 最小二乘法（最小化MSE）的解析解可通过矩阵运算直接求出
       - **若误差服从正态分布，则均方误差对应极大似然估计，是最优的损失函数**
   - 平均绝对误差MAE
     - 特点：
       - 平均绝对误差受异常值影响较小，但对小误差的惩罚较弱
       - 适用与数据中存在显著异常值（如金融风险预测）的场景
2. 线性方程求解方法：【️因为选择了均方误差作为损失函数，函数可导】
   - 【解析法】一元线性回归解析解：最小二乘法
   - 【解析法】正规方程法：基于最小二乘法的计算方案，通过求解矩阵方程来直接获得参数值
     - 正规方程法适用于特征数量较少的情况。当特征数量较大时，计算逆矩阵的复杂度会显著增加，此时梯度下降法更为适用
     - 代码实战
       ```python
       # fit_intercept: 是否计算偏置，也就是是否需要一个截距来拟合模型，默认是需要的
       model = sklearn.linear_model.LinearRegression(fit_intercept=True)
       model.fit([[0, 3], [1, 2], [2, 1]], [0, 1, 2])
       # coef_: 系数
       print(model.coef_)
       # intercept_: 偏置
       print(model.intercept_)
       ```
   - 【迭代法】梯度下降法
     - 是一种用于最小化目标函数的迭代优化算法。核心是沿着目标函数（如损失函数）的负梯度方向逐步调整参数，从而逼近函数的最小值。梯度方向指示了函数增长最快的方向，因此负梯度方向是函数下降最快的方向
     - 梯度接近0或者迭代次数增加时，迭代完毕
     - 梯度下降法常见问题
       - 特征缩放：通常需要提前对特征进行缩放（如标准化或归一化），以加快收敛速度
       - 局部极小值、鞍点问题
         - 可能陷入局部极小值（非全局最优解），或遇到鞍点（梯度为零但非极值点）
         - 解决方案：使用动量（Momentum）、自适应优化器（如Adam）或二阶方法（如牛顿法）
3. 数值计算方法
   - 解析法：有公式，使用公式求解
   - 迭代法：没有公式，通过多次迭代逐渐逼近最优解
   - 数值逼近法：用简单函数（多项式、样条）去近似代替复杂函数，方便计算
   - 数值微分：函数求导、积分没有解析解时，用离散点近似算
   - 蒙特卡洛方法：使用大量随机采样来估算结果
   - ...
4. **学习率的选择**
   - 学习率过大：可能导致跳过最优解，甚至发散
   - 学习率过小：收敛速度慢，容易陷入局部极小
   - 自适应学习率：高级优化器动态调整学习率以提升性能

## 四、实战
1. 简单实战
   ```python
   from sklearn.linear_model import LinearRegression
   # 自变量，每周学习时长
   X = [[5], [8], [10], [12], [15], [3], [7], [9], [14], [6]]
   # 因变量，数学考试成绩
   y = [55, 65, 70, 75, 85, 50, 60, 72, 80, 58]
   # 实例化线性回归模型
   model = LinearRegression()
   # 模型训练
   model.fit(X, y)
   # 系数，每周每学习1小时，成绩会增加多少分
   print(model.coef_)
   # 截距
   print(model.intercept_)
   # 预测,每周学习11小时，成绩可能是多少分
   print(model.predict([[11]]))
   ```
2. 使用梯度下降法进行线性回归训练模型：大批量数据的时候使用随机梯度下降
   ```python
   from sklearn.linear_model import SGDRegressor

   # 定义线性回归模型，使用随机梯度下降迭代
   model = sklearn.linear_model.SGDRegressor(
       penalty=None, # 【正则化】正则化项，用于惩罚参数过大
       alpha=0.0001, # 【正则化】正则化系数，不是学习率
       l1_ratio=0.15, # 【正则化】l1正则化和l2正则化的比例系数
       loss="squared_error",  # 损失函数，默认为均方误差
       fit_intercept=True,  # 是否计算偏置，是否计算出一个截距
       learning_rate="constant",  # 学习率策略
       eta0=0.1,  # 初始学习率
       max_iter=1000,  # 最大迭代次数
       tol=1e-8,  # 损失值变化量小于tol时停止迭代，迭代停止阈值
   )
   model.fit([[0, 3], [1, 2], [2, 1]], [0, 1, 2])
   # coef_: 系数
   print(model.coef_)
   # intercept_: 偏置
   print(model.intercept_)
   ```
3. 广告投放效果case：见(./code&data/linear/advertising.py)代码实战
   - 数据集
   - 特征工程
   - 模型训练与评估
   - 交叉验证

参考资料：
1. 尚硅谷机器学习视频：https://www.bilibili.com/video/BV1BYe4z5E9z