实践课一

## 01 | 软件建模与文档:架构师怎样绘制系统架构蓝图?
我们主要的手段就是软件建模，以及将这些软件模型组织成一篇有 价值的软件设计文档。
### 软件建模
不同的开发工程师会清晰自己开发的模块和其他同事工作内容的关系与依赖，并按照这些模型开发代码。
如何建模？——>两个客观存在
* 一个是我们要解决的领域问题
* 另一个客观存在就是最终开发出来的软件系统（组成、依赖、调用、部署、通信等）

对领域问题和软件系统进行分析、设计和抽象的这个过程，就是软件建模设计。
### 软件设计方法
UML 包含的软件 模型有 10 种，其中常用的有 7 种:类图、序列图、组件图、部署图、用例图、状态图和 活动图。
（推荐你阅读马丁富勒的《UML 精粹》一书）
#### 类图
用来描述类的特性和类之间的静态关系。
一个类包含三个部分:类的名字、类的属性列表和类的方法列表。类之间有 6 种静态关 系:关联、依赖、组合、聚合、继承、泛化。
#### 时序图
用来描述参与者之间的动态调用关系。
每个参与者有一条垂直向下的生命线，这条线用虚线表示。而参与者之 间的消息从上到下表示其调用的前后顺序关系
只要是描述**不同参与者**之间交互的，都可以使用时序图。
#### 组件图
组件图描述组件之间的静态关系，主要是依赖关系，如果你想要描述组件之间的动态调用关系，可以使用组件时序图，以组件作为参与者，描述组件之间的消息调用关系。
#### 部署图
是在设计早期就需要画的一种模型图。根据部署图，各方可以讨论对这个方案是否认可
#### 用例图
通过反映用户和软件系统的交互，描述系统的功能需求。
#### 状态图
状态图用来展示单个对象生命周期的状态变迁。
#### 活动图
主要用来描述过程逻辑和业务流程。UML 中没有流程图，很多时候，人们用活动图代替流程图。
实心圆代表流程开始，空心圆代表流程结束，圆角矩形表示活动，菱形表示分支判断。
泳道：活动图可以根据活动的范围，将活动根据 领域、系统和角色等划分到不同的泳道中，使流程边界更加清晰
### 软件设计文档
软件设计过程可以拆分成需求分析、概要设计和详细设计三个阶段。
#### 需求分析阶段
用例图：描述系统的功能与使用场景;
活动图：对于关键的业务流程
时序图：描述新系统和原来的子系统的调用关系
状态图：某些对象内部会有复杂的状态变化
#### 概要设计阶段
部署图：描述系统最终的物理蓝图
组件图以及组件时序图：设计软件主要模块及其关系
组件活动图：描述组件间的流程逻辑。
#### 详细设计阶段
类图和类的时序图：指导最终的代码开发，如果某个类方法内部有比较复杂的逻辑，那么可以将这个方法的逻辑用活动图进行描述。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773309702988.jpg)


**架构师应该针对不同的相关方， 使用不同的模型图输出不同的架构文档。**

## 02 | 高并发架构设计方法:面对高并发，怎么对症下药?
### 高并发系统架构的方法论
核心就是为了满足用户的高并发访问，系统需要提供更多的计算资源。
解决方案大致可以分成两类，一类是传统大型软件系统的技术方案，被称作垂直伸缩方案。所谓的垂直伸缩就是提升单台服务器的处理能力
另一类解决方案，水平伸缩，指的是使用更多的服务器，将这些服务器构成一个分布式集群，通过这个集群，对外统一提供服务，以此来提高系统整体的处理能力。
水平伸缩除了还有一个天然的好处，那就是具有更好的弹性。因此现在的大型互联网系统多采取水平伸缩方案，来应对用户的高并发访问。
### 高并发系统架构的方法
主要技术方法，其核心是各种分布式技术。
#### 分布式应用
主要手段就是使用负载均衡服务器，将多台应用服务器构成一个分布式集 群，用户请求首先到达负载均衡服务器，然后由负载均衡服务器将请求分发到不同的应用 服务器上
#### 分布式缓存
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773309390979.jpg)
#### 分布式消息队列
分布式消息队列是解决突发的高并发写操作问题和实现更简单的集群伸缩的一种常用技术方案。消息队列架构主要包含三个角色:消息生产者、消息队列、消息消费者
#### 分布式关系数据库
为了解决关系数据库存储海量数据以及提供高并发读写的问题，人们提出了将数据进行分片，再将 不同分片写入到不同数据库服务器的方法。
#### 分布式微服务
微服务的核心思想是**将单体架构中庞大的业务逻辑拆分成一些更小、更低耦合的服务，然 后通过服务间的调用完成业务的处理。**

微服务架构的实现需要依赖一个微服务框架，这个框架包括一个微服务注册中心和一个 RPC 远程调用框架。微服务客户端通过注册中心得到要调用的微服务具体的地址列表，然 后通过一个软负载均衡算法选择其中一个服务器地址，再通过 PRC 进行远程调用。
除了以上这些分布式技术，高并发系统中常用的还有大数据、分布式文件、区块 链、搜索引擎、NoSQL、CDN、反向代理等技术，也都是一些非常经典的分布式技术。
### 系统并发指标
* 目标用户数
* 系统用户数
* 活跃用户数
* 在线用户数
* 并发用户数

有了上面这些用户数指标，我们就可以进一步估算架构设计需要考虑的其他一些技术指标，比如每天需要新增的文件存储空间，存储总系统用户需要的数据库规模，总网络带宽，每秒处理的请求数等等。
## 03 | 短 URL 生成器设计:百亿短 URL 怎样做到无冲突?
我们将设计开发一个短 URL 生成器，产品名称是“Fuxi(伏 羲)”。
我们预计 Fuxi 需要管理的短 URL 规模在百亿级别，并发吞吐量达到数万级别。这个量级 的数据对应的存储方案是什么样的?用传统的关系数据库存储，还是有其他更简单的办 法?此外，如何提升系统的并发处理能力呢?这些是我们今天要重点考虑的问题。
### 需求分析
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773318885727.jpg)
#### 短URL生成器的用例图
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773319398064.jpg)
#### 性能指标估算
* 存储容量和并发量
* 存储空间
* 吞吐量
* 网络带宽
    * 一般系统高峰期访问量是平均访问量的 2 倍
* 网络带宽
* 短URL长度估算
    * 646 ≈ 680亿，按我们前面评估，总 URL 数 120 亿，6 个字符的编码就可以满足需求。因此 Fuxi 的短 URL 编码长度 6 个字符
#### 非功能需求
1. 系统需要保持高可用，不因为服务器、数据库宕机而引起服务失效。
2. 系统需要保持高性能，服务端 80% 请求响应时间应小于 5ms，99% 请求响应时间小于 20ms，平均响应时间小于 10ms。
3. 短 URL 应该是不可猜测的，即不能猜测某个短 URL 是否存在，也不能猜测短 URL 可能 对应的长 URL 地址内容。
### 概要设计
设计核心就是短 URL 的生成。即长 URL 通过某种函数，计算得到一个6个字符的短URL。
#### 单项散列函数生成短 URL
通常的设计方案是，将长 URL 利用 MD5 或者 SHA256 等单项散列算法，进行 Hash 计 算，得到 128bit 或者 256bit 的 Hash 值。然后对该 Hash 值进行 Base64 编码，得到 22 个或者 43 个 Base64 字符，再截取前面的 6 个字符，就得到短 URL 了，如图。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773356757920.jpg)
由于可能Hash冲突，所以在生成的时候，需要先校验该短 URL 是否已经映 射为其他的长 URL，如果是，那么需要重新计算
这样的冲突处理需要多次到存储中查找 URL，无法保证 Fuxi 的性能要求。
#### 自增长短 URL
一种免冲突的算法是用自增长自然数来实现，即维持一个自增长的二进制自然数，然后将 该自然数进行 Base64 编码即可得到一系列的短 URL。这样生成的的短 URL 必然唯一
这种算法导致短URL可猜测，某时间段内的短链会集中在一个区间（可猜测）。Fuxi 的需求是不允许短URL可预测。
#### 预生成短URL
先生成一批没有冲突的短 URL 字符串，当外 部请求输入长 URL 需要生成短 URL 的时候，直接从预先生成好的短 URL 字符串池中获取 一个即可。
预生成短 URL 的算法可以采用随机数来实现，6 个字符，每个字符都用随机数产生(用 0~63 的随机数产生一个 Base64 编码字符)。为了避免随机数产生的短 URL 冲突，需要 在预生成的时候检查该 URL 是否已经存在(用布隆过滤器检查)。因为预生成短 URL 是 离线的，所以这时不会有性能方面的问题。
#### Fuxi的整体部署模型
相对比较有挑战的就是高并发的读请求如何处理、预生成的短URL如何存储以及访问。高并发访问主要通过负载均衡与分布式缓存解决，而海量数据存储则通过HDFS以及HBase来完成。具体架构图如下。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773360430146.jpg)
系统调用可以分成两种情况，一种是用户请求生成短URL的过程;另一种是用户访问短URL，通过Fuxi跳转到长URL的过程。
* 对于用户请求生成短 RUL 的过程，时序图如下
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773362145247.jpg)
* 对于用户通过客户端请求访问短 URL 的过程(即输入短 URL，请求返回长URL)，时序图如下
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773363818928.jpg)
* 为了保证系统高可用，Fuxi 的应用服务器、文件服务器、数据库服务器都采用集群部署方 案，单个服务器故障不会影响 Fuxi 短 URL 的可用性。
* 对于 Fuxi 的高性能要求，80% 以上的访问请求将被设计为通过缓存返回。Redis 的缓存响 应时间 1ms 左右，服务器端请求响应时间小于 3ms，满足 80% 请求小于 5ms 的性能目 标。对于缓存没有命中的数据，通过 HBase 获取，HBase 平均响应时间 10ms，也可以满 足设计目标中的性能指标。
* 对于 Redis 缓存内存空间估算，业界一般认为，超过 80% 请求集中在最近 6 天生成的短 URL 上，Fuxi 主要缓存最近六天生成的短 URL 即可。根据需求容量估计，最近 6 天生成 的短 URL 数量约 1 亿条，因此需要 Redis 缓存服务器内存空间:1亿 × 1KB = 100GB。
### 详细设计
详细设计关注重定向响应码、短 URL 预生成文件及加载、用户自定义短URL等几个关键设计点。
#### 重定向响应码
* 其中 301 表示永久重 定向，即浏览器一旦访问过该短 URL，直接根据缓存在浏览器(HTTP 客户端)的长 URL 路径进行访问。
* 302 表示临时重定向，每次访问短 URL 都需要访问短 URL 生成器。

Fuxi 的架构设计完全可以承受这些负载压力，因此 Fuxi 使用 302 状态码构造重 定向响应。
#### 短URL预生成文件及预加载
Fuxi 的短 URL 是在系统上线前全部预生成的，并存储在 HDFS 文件中。共 144 亿个短URL，每个短 URL 6 个字符，文件大小 144亿 × 6B = 86.4GB。

由于预加载短 URL 服务器集群部署 多台服务器，应对同时加载相同短 URL 的情况，利用偏移量文件 对多个服务器进行互斥操作，即**利用文件系统写操作锁的互斥性实现多服务器访问互斥。**

应用程序的文件访问流程应该是:写打开偏移量文件 -> 读偏移量 -> 读打开短 URL 文件 - > 从偏移量开始读取 60K 数据 -> 关闭短 URL 文件 -> 修改偏移量文件 -> 关闭偏移量文件。

加载到预加载短 URL 服务器的 1 万个短 URL 会以链表的方式存储，每使用一个短 URL， 链表头指针就向后移动一位，并设置前一个链表元素的 next 对象为 null。这样用过的短 URL 对象可以被垃圾回收。

当剩余链表长度不足 2000 的时候，触发一个异步线程，从文件中加载 1 万个新的短 URL，并链接到链表的尾部。
#### 用户自定义短 URL
Fuxi 限制用户自定义短 URL 的字符个 数，不允许用户使用 6 个字符的自定义短 URL，且 URL 长度不得超过 20 个字符。

生成自定义短 URL 的时候需要到数据库中检查冲突，是否指定的 URL 已经被使用，如果发生冲突，要求用户重新指定。
#### URL Base64 编码
其中“+”和“/”在 URL 中会被编码为“%2B”以及“%2F”，而“%”在写入数据库的 时候又和 SQL 编码规则冲突，需要进行再编码。

使用 URL 保留字符表以外的字 符对 Base64 编码表中的 62，63 进行编码:将“+”改为“-”，将“/”改为“_”
（这块有建议使用Base62编码，因为-和_字符在链接首尾显得有点突兀）

## 04 | 网页爬虫设计:如何下载千亿级网页?
### 需求分析
#### 性能指标估算
* 每月新增存储量
* 总存储空间
* TPS
#### 非功能需求
* 伸缩性：Bajie 可以灵活部署，扩大集群规模，增强其爬取网页的速度。也就是说，Bajie 必须是一个分布式爬虫。
* 健壮性：能够面对各种异常，正常运行。
* 去重：URL去重、内容去重
* 扩展性：当前只需要爬取 HTML 页面即可，将来可能会扩展到图片、视频、文档等内容页面。

Bajie 必须是“礼貌的”。Bajie 要避免对同一个域名进行并发爬取，还要根据目标服务器的承载能力增加访问延迟，即在两次爬取访问之间，增加等待时间

Bajie 还需要遵循互联网爬虫协议，即目标网站的 robots.txt 协议，不爬取目标网站 禁止爬取的内容
### 概要设计
**将遍历到的网页下载保存起来**，就是爬虫的主要工作。
Bajie 不需要事先知道数千亿的 URL。Bajie 只需要知道一小部分 URL，也就是所谓的种子 URL。处理流程图如下
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773388181593.jpg)
其中，**URL 调度器是整个爬虫系统的中枢和核心**，也是整个爬虫的驱动 器。爬虫就是靠着 URL 调度器源源不断地选择 URL，然后有节奏、可控地下载了整个互联 网，所以 **URL 调度器也是爬虫的策略中心。**
Bajie的部署图如下
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773389267498.jpg)
### 分布式爬虫
#### 详细设计
Bajie 详细设计关注 3 个技术关键点:URL 调度器算法、去重算法、高可用设计。
##### URL 调度器算法
URL 集合数量会随着页面不断下载而指数级增加。待下载 URL 数量将远远大于系统的下载能力，**URL 调度器就需要决定当前先下载哪些 URL。**

深度优先需要维护较为复杂的数据结构，而且太深的下载深度导致下载的页面非常分散，不利于我们构建搜索引擎和数据分析。所以我们没有使用深度优先算法。

使用队列的先进先出实现广度优先算法
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773392257849.jpg)
爬虫应该优先爬取那些高质量 的网站。优先级和域名都可以使用不同队列来区分
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773392547803.jpg)
##### 去重算法
URL 去重可以使用布隆过滤器以提高效率。

Bajie 计算页面内容的 MD5 值，通过判断下载页面的内容 MD5 值是否已经存在，判断内 容是否重复。（比较内容重复的时候，需要将 HTML 里面的有效内容提取出来，也就是提取出去除 HTML 标签的文本信息，针对有效内容计算 MD5）。
查询MD5是否存在时，用布隆过滤器代替 Hash 表，以优化性能。
##### 高可用设计
不需要像一般互联网系统那样进行高可用设计

URL 调度器和 URL 下载处理服务器都需要记录运行时状态，即存储本服务器已经加 载的 URL 和已经处理完成的 URL，这样宕机恢复的时候，就可以立刻读取到这些状态数据，进而使服务器恢复到宕机前的状态。对于 URL 下载处理服务器，Bajie 采用 Redis 记 录运行时状态数据。

对于 URL 下载处理服务器，Bajie 采用 Redis 记 录运行时状态数据。

URL 下载处理服务器会采用多线程(池)设 计。每个线程独立完成一个 URL 的下载和处理，线程也需要捕获各种异常，不会使自己因为网络超时或者解析异常而退出。

> 对于一个千亿级网页的爬虫系统而言，最主要的技术挑战应该是海量文件的存储与计算

## 05 | 网盘系统设计:万亿 GB 网盘如何实现秒传与限速?
网盘的主要技术挑战是海量数据的高并发读写访问
### 需求分析
DBox 的核心功能是提供文件上传和下载服务。

DBox 需要对上传和下载进行**流速控制**，保证付费用户得到更多的网络资源
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773396376094.jpg)
#### 负载指标估算
DBox 的设计目标是支持 10 亿用户注册使用，免费用户最大可拥有 1TB 存储空间。预计 日活用户占总用户的 20%，即 2 亿用户。每个活跃用户平均每天上传、下载 4 个文件。

DBox 的存储量、吞吐量、带宽负载估算如下。
* 总存储量
10亿 × 1T B = 10亿T B，去重后真正需要的存储空间大约是这个估算值的 10%
* QPS
系统需要满足的平均 QPS 约为 10000。
2亿×4÷(24×60×60)≈ 1万
高峰期 QPS 约为平均 QPS 的两倍，即 2 万。
* 带宽负载
每次上传下载文件平均大小 1MB，所以需要网络带宽负载 10GB/s，即 80Gb/s。
1万× 1MB = 10GB/s = 80Gb/s 同样，高峰期带宽负载为 160Gb/s。

#### 非功能需求
1. 大数据量存储:10 亿注册用户，1000 亿个文件，约 1 亿 TB 的存储空间。
2. 高并发访问:平均 1 万 QPS，高峰期 2 万 QPS。
3. 大流量负载:平均网络带宽负载 80Gb/S，高峰期带宽负载160Gb/s。
4. 高可靠存储:文件不丢失，持久存储可靠性达到 99.9999% ，即 100 万个文件最多丢失 (或损坏)1 个文件。
5. 高可用服务:用户正常上传、下载服务可用性在 99.99% 以上，即一年最多 53 分钟不可用。 6. 数据安全性:文件需要加密存储，用户本人及共享文件外，其他人不能查看文件内容。
7. 不重复上传:相同文件内容不重复上传，也就是说，如果用户上传的文件内容已经被其他用户上传过了，该用户不需要再上传一次文件内容，进而实现“秒传”功能。从用户视角来看，不到一秒就可以完成一个大文件的上传。
### 概要设计
网盘设计的关键是**元数据与文件内容的分离存储与管理**
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773403522799.jpg)
DBox 采用对象存储作为最终的文件存储方案，而对象存储不适合 存储大文件，需要进行切分。而大文件进行切分还带来其他的好处:可以**以 Block 为单位进行上传和下载，提高文件传输速度**;客户端或者网络故障导致文件传输失败，也只需要 重新传输失败的 Block 就可以，进而实现**断点续传**功能。

用户上传文件的时序图如下。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773404365497.jpg)
类似的，用户下载文件的时序图如下。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773404598768.jpg)
### 详细设计
##### 元数据库设计
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773405911692.jpg)
其中，User 表和 File 表为一对多的关系，File 表和 Block 表也是一对多的关系。

这 3 种表的记录数都是百亿级以上，所以元数据表采用分**片的关系数据库**存储。

因为查询的主要场景是根据用户 ID 查找用户信息和文件信息，以及根据文件 ID 查询 block 信息，所以 User 和 File 表都采用 user_id 作为分片键，Block 表采用 file_id 作为 分片键。
##### 限速
API 服务器可以 根据用户类型，决定分配的 Block 服务器数目和 Block 服务器内的服务线程数，以及每个线程的上传、下载速率。

Block 服务器会根据 API 服务器的返回值，来控制客户端能够同时上传、下载的 Block 数 量以及传输速率，以此对不同用户进行限速。
##### 秒传
DBox 需要通过更多信息判断文件是否相同:**只有文件长度、文件开头 256KB 的 MD5 值、文件的 MD5 值，三个值都相同，才会认为文件相同**。当文件长度小于 256KB，则直接上传文件，不启用秒传功能。

将原来的 File 表拆分成物理文件 表 Physics_File 和逻辑文件表 Logic_File。其中，Logic_File 记录用户文件的元数据，并和物理文件表 Physics_File 建立多对 1 关联关系，而 Block 表关联的则是 Physics_File 表，如下。

![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773411739802.jpg)

Logic_File 中字段 double_md5 记录了文件头 256KB 的 MD5、文件 MD5 两个数据拼接 后的数据，而 size 记录了文件长度，只有这两个字段都相同才会启用秒传。

> 应用架构师需要掌握的技术栈更加**广泛**，要能够掌握各种基础设施技术的特性，并能根据 业务特点选择最合适的方案;而基础设施架构师需要的技术栈更加**深入**

## 06 | 短视频系统设计:如何支持三千万用户同时在线看视频?
我们准备开发一个面向全球用户的短视频应用，用户总量预计 20 亿，应用名称: QuickTok。

QuickTok 的主要技术挑战是:如何应对高并发用户访问时的网络带宽压力，以及如何存储海量的短视频文件。
### 需求分析
QuickTok 的核心功能需求非常简单:用户上传视频、搜索视频、观看视频。我们将主要分析非功能需求。

* 日活用户->日播放量->平均播放 QPS ->同时在观看的视频数
* 平均需要每秒上传视频数->每秒上传至服务器的文件大小->每年新增视频需要的存储空间->总带宽

为了保证视频数据的高可用，不会因为硬盘损坏导致数据丢失，视频文件需要备份存储，QuickTok 采用双副本的备份存储策略，也就是每个视频文件存储三份

我们需要设计的短视频应用是一个每秒上传 550 个视频文件、11 万次播放、新增 165GB 存储以及 88Tb 总带宽的**高并发**应用系统。这个系统呢需要是**高性能**的，能迅速响应用户的上传和播放操作，也需要是**高可用**的，能面向全球用户提供 7 * 24 小时稳定的服务。
### 概要设计
核心部署模型如下图。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773419144792.jpg)
视频内容处理器是一个由责任链模式构建起来的管道。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773428810076.jpg)
视频上传环节的具体时序图如下。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773429081322.jpg)
对视频搜索及播放部分的设计，即核心部署模型图中标红的部分，如下。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773431914479.jpg)
**视频搜索引擎**会根据用户提交的视频标题、上传用户等元数据，以及视频内容处理器生成 的内容标签构建**倒排索引**。

当用户点击缩略图时，App 开始播放视频。App 并不需要下载完整个视频文件才开始播放，而是**以流的方式一边下载视频数据，一边播放**。QuickTok 使用 MPEG–DASH 流媒体传输协议进行视频流传输，因为这个协议具 有自适应能力，而且支持 HTTP，可以应对 QuickTok 的视频播放需求。

### 详细设计
详细设计将关注视频存储系统、性能优化与 CDN。
#### 视频存储系统设计
可以尝试与网盘相同的存储技术 方案，将视频文件拆分成若干 block，使用对象存储服务进行存储。

但 QuickTok 最终采用了另一种存储方案，即使用 Hadoop 分布式文件系统 HDFS 进行 存储。HDFS 适合大文件存储的一次写入多次读取的场景，满足视频一次上传多次播放的 需求;同时，它还可以自动进行数据备份(缺省配置下，每个文件存储三份)，也满足我 们关于数据存储高可用的需求。

HDFS 适合存储大文件，大文件减少磁盘碎片，更有利于存储空间的利用，同时 HDFS NameNode 的访问压力也更小，所以我们需要把若干个视频文件合并成一个 HDFS 文件 进行存储，并将存储相关的细节记录到 HBase 中。
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773435118363.jpg)
#### 性能优化与 CDN 设计
如果用户的大部分请求都可以通过 CDN 返回，那么一方面可以极大**加快用户请求的响应速度**，另一方面又可以较大**缓解数据中心的网络和硬盘负载压力**，进一步提升应用整体的性能。

通常的 CDN 设计，是在 CDN 中没有用户请求的数据时，进行回源，即由 CDN 请求数据 中心返回需要的数据，然后缓存在 CDN 本地。

但 QuickTok 考虑到了短视频的特点:大 V、网红们发布的短视频会被更快速、更广泛地 播放。因此针对粉丝量超过 10 万的用户，系统将采用**主动推送 CDN** 的方法，以提高 CDN 的命中率，优化用户体验，如图:
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773436359633.jpg)
如果是 10 万粉丝以上的用户 发布了短视频，CDN 推送服务会根据其粉丝活跃的区域，将视频推送到对应区域的 CDN 服务器上。

短视频的完播率通常不足 30%，所以 QuickTok 也不需要将完整视频推送到 CDN，只需 要根据视频发布者的历史播放记录，计算其完播率和播放期望进度，然后将短视频切分成 若干 chunk，将部分 chunk 推送到 CDN 即可。

业界一般共识，**视频应用 CDN 处理的带宽大约占总带宽的 95% 以上**，也就是说，通过合 理使用 CDN，QuickTok 数据中心需要处理的带宽压力不到 4Tb。
#### 缩略图生成与推荐设计
我们需要通过**大数据平台的机器学习引擎**来完成缩略图的生成和推荐，如下图
![](https://myupic-1253836703.cos.ap-nanjing.myqcloud.com/other_pics/2023/02/16773437984272.jpg)
缩略图的生成和推荐可以分为两个具体过程:
* 实时在线的缩略图推荐过程 a;
    * 推荐引擎可以获取当前用户的偏好特征标签以及视频对应的多个缩略图特征，使用 XGboost 算法训练好的模型，将用户特征标签和缩略图特征进行匹配，然后返回最有可能 被当前用户点击的缩略图 ID。
* 利用离线机器学习生成优质缩略图的过程 b。
    * 机器学习系统获取到了海量用户的浏览和点击数据，同时获取每个缩略图的特征。一方面，机器可以学习到，哪些特征的缩略图更容易获得用户点击，从而生成优质缩略图特征标签库;另一方面，机器还可以学习到每个用户自身更偏好的图像特征标签，供前面提到的推荐引擎使用。
    * 视频内容处理器可以使用随机的办法，抽取一些帧作为缩略图，进行冷启动。

> 关于视频转码的两个场景：一种是用户上传的不同的视频编码格式需要统一转码成平台格式;一种是转码成不同清晰度的视频文件，根据用户带宽和会员等级进行传输。



-------------------------------------------------------------------
实践课二

## 07 | 海量数据处理技术回顾:为什么分布式会遇到 CAP 难题?
因为相对于“无状态”(stateless)的计算逻辑(可执行程序)，数据存储是“有状态”(stateful)的。

海量数据存储的核心问题包括:如何利用分布式服务器集群实现海量数据的统一存储?如何正确选择服务器写入并读取数据?为了保证数据的高可用性，如何实现数据的多备份存储?数据多备份存储的时候，又如何保证数据的一致性?
### HDFS
HDFS，即 Hadoop 分布式文件系统，其架构如下。
![](media/16773698371335/16773700658891.jpg)
HDFS 的关键组件有两个，一个是 NameNode，另一个是 DataNode。
NameNode 负责整个分布式文件系统的元数据管理，也就是文件路径名、访问权限、数据 块 ID、存储位置等信息。而 DataNode 负责文件数据的存储和读写操作，HDFS 将文件数 据分割成若干数据块(Block)，每个 DataNode 存储一部分数据块，这样文件就分布存 储在了整个 HDFS 服务器集群中。

HDFS 为了高可用，会将一个数据块复制为多份(缺省情况为 3 份)，并将多 份相同的数据块存储在不同的服务器上，甚至不同的机架上。

HDFS 的典型应用场景是大数据计算，即使用 MapReduce、Spark 这样的计算框架来计 算存储在 HDFS 上的数据。但是作为一个经典的分布式文件系统，我们也可以把 HDFS 用 于海量文件数据的存储与访问。

### 分布式关系数据库
可以使用分布式关系数据库中间件来解决这个问题，在中间件中完成数据的分片逻 辑，这样对应用程序是透明的。我们常用的分布式关系数据库中间件是 MyCAT，原理如下图。
![](media/16773698371335/16773704256623.jpg)

MyCAT 是针对 MySQL 数据库设计的，应用程序可以像使用 MySQL 数据库一样连接 MYCAT，提交 SQL 命令。MyCAT 在收到 SQL 命令以后，查找配置的分片逻辑规则。

### HBase
HBase 架构如下。
![](media/16773698371335/16773704963579.jpg)
HRegion 是 HBase 中负责数据存储的主要进程，应用程序对数据的读写操作都是通过和 HRetion 通信完成的。

HBase 的设计重点就是 HRegion 的分布式。HRegionServer 是物理服务器，这些服 务器构成一个分布式集群，每个 HRegionServer 上可以启动多个 HRegion 实例。当一个 HRegion 中写入的数据太多，达到配置的阈值时，一个 HRegion 会分裂成两个 HRegion，并将 HRegion 在整个集群中进行迁移，以使 HRegionServer 的负载均衡，进 而实现 HRegion 的分布式。

应用程序要 先访问 HMaster 服务器，得到数据 key 所在的 HRegion 信息，再访问对应的 HRegion 获取数据。为了保证 HMaster 的高可用，HBase 会启动多个 HMaster，并通过 ZooKeeper 选举出一个主服务器。
### ZooKeeper
CAP 原理认为，一个提供数据服务的分布式系统无法同时满足数据一致性 (Consistency)、可用性(Availibility)、分区耐受性(Patition Tolerance)这三个条件。

由于互联网对高可用的追求，大多数分布式存储系统选择可用性，而放松对一致性的要 求。而 ZooKeeper 则是一个保证数据一致性的分布式系统，它主要通过一个 ZAB 算法 (Zookeeper Atomic Broadcast， Zookeeper 原子广播)实现数据一致性，算法过程如 下。
![](media/16773698371335/16773708399520.jpg)
### 布隆过滤器
布隆过滤器首先开辟一块巨大的连续内存空间，并将这个空间所有比特位都设置为 0。然后对每个 MD5 使用多种 Hash 算法，比如使用 8 种 Hash 算法，分别计算 8 个 Hash 值，并保证 每个 Hash 值是落在这个 1600G 的空间里的，也就是，每个 Hash 值对应 1600G 空间里的一个地址下标。然后根据计算出来的 Hash 值将对应的地址空间里的比特值设为 1，这 样一个 MD5 就可以将 8 个比特位设置为 1。
## 08 | 秒杀系统设计:你的系统可以应对万人抢购盛况吗?
Apollo 的核心挑战是:如何应对突然出现的数百倍高并发访问压力，并保 证用户只有在秒杀开始时才能下单购买秒杀商品?
### 需求分析
Apollo 的需求主要有两点。
* 独立开发部署秒杀系统，避免影响现有系统和业务
    * 秒杀业务不能使用正常的电商业务流程，也不能和正常的网站交易业务共用服务器，甚至域名也需要使用自己独立的域名。
* 防止跳过秒杀页面直接下单
    * 在此时间点之前，只能浏览 商品信息，不能下单。避免拿到URL提前下单
### 概要设计
Apollo 要解决的核心问题有:
1. 如何设计一个独立于原有电子商务系统的秒杀系统，并独立部署。
2. 这个秒杀系统如何承受比正常情况高数百倍的高并发访问压力。
3. 如何防止跳过秒杀页面获得下单 URL。 我们将讨论这三个问题的解决方案，并设计秒杀系统部署模型。
#### 独立秒杀系统页面设计
#### 秒杀系统的流量控制
缓存是提高响应速度、降低服务器负载压力的重要手段。Apollo 采用多级缓存方案，可以更有效地降低服务器的负载压力。
* 首先，浏览器尽可能在本地缓存当前页面，页面本身的 HTML、JavaScript、CSS、图片等 内容全部开启浏览器缓存。
* 其次，秒杀系统还使用 CDN 缓存。
* 秒杀系统中提供 HTML、JavaScript、CSS、图片的静态资源服务器和提供商品浏览的秒杀商品服务器也要在本地开启缓存功能。
![](media/16773698371335/16773715088079.jpg)

需要在用户提交订单时，检查是否已经有其他用户提交订单。

事实上，为了减轻下单页面服务器的负载压力，可以控制进入下单页面的入口，只有少数用户能进入下单页面，其他用户则直接进入秒杀结束页面。
#### 秒杀商品页面购买按钮点亮方案设计与下单 URL 下发
需要在秒杀商品静态页面中加入一个特殊的 JavaScript 文件，这个 JavaScript 文件设置为不被任何地方缓存。秒杀未开始时，该 JavaScript 文件内容为空。当秒杀开始 时，定时任务会生成新的 JavaScript 文件内容，并推送到 JavaScript 服务器。

新的 JavaScript 文件包含了秒杀是否开始的标志和下单页面 URL 的随机数参数。

这个 JavaScript 文件还有一个优点，那就是它本身非常小，即使每次浏览器刷新都访问 JavaScript 文件服务器，也不会对服务器集群和网络带宽造成太大压力。
### 秒杀系统部署模型
![](media/16773698371335/16773718343190.jpg)
用户刷新页面时，除了特殊 JavaScript 文件，其他页面和资源文件都可以通过缓存获得。
下单 URL 中会包含一个随机数，这个随机数也会由定时任务推送给下单服务器，下单服务器收到用户请求的时候，检查请求中包含的随机数是否正确，即检查该请求是否是伪造的。
## 09 | 交友系统设计:哪种地理空间邻近算法更快?
Liao 面临的技术挑战包括:面对海量的用户，如何为其快速找到邻近的人，可以选择的地 理空间邻近算法有哪些?Liao 如何在这些算法中选择出最合适的那个?
### 需求分析
功能用例图如下
![](media/16773698371335/16773722788290.jpg)
#### 用户规模分析
Liao 的目标用户是全球范围内的中青年单身男女，预估目标用户超过 10 亿，系统按 10 亿 用户进行设计。
### 概要设计
采用典型的微服务架构设计方案，用户通过网关服务器访问具体的微服 务。
![](media/16773698371335/16773723255029.jpg)
首先，用户所有请求都通过统一的**网关服务器**处理。网关服务器负责限流、 防攻击、用户身份识别及权限验证、微服务调用及数据聚合封装等，而真正的业务逻辑则 通过访问微服务来完成。

* 用户微服务
* 图片微服务
    * 我们使用 Nginx 作为图片服务器，图片服务器可以线性扩容，每写满一台 服务器(及其 Slave 服务器)，就继续写入下一台服务器。服务器 IP、图片路径则记录在 用户数据库中。同时，购买 CDN 服务，缓存热门的用户照片。
* 配对微服务
    * 将数据发送给一个流式大数据引擎进行计算。
* 推荐微服务：负责向用户展示其可能感兴趣的、邻近的用户
    * 一方面，推荐微服务需 要根据用户操作、个人兴趣、交友偏好调用协同过滤等推荐算法进行推荐，另一方面必须 保证推荐的用户在当前用户的附近。
### 详细设计
距离计算公式采用半正矢公式。

通常的空间邻近算法有以下 4 种，我们一一进行分析，最终选择出最合适的方案。
* SQL 邻近算法（直接毙掉）
* 地球网格临近算法
    * 我们可以将所有的网格及其包含的用户都记录在内存中。当我们进行邻近查询时，只需要 在内存中计算用户及其邻近的 8 个网格内的所有用户的距离即可。
* 动态网格算法
    * 将 4 叉树所有的叶子节点顺序组成一个双向链表，每个节点在链表上的若 干个前驱和后继节点正好就是其地理位置邻近的节点。
    * 动态网格也叫 4 叉树网格，在空间邻近算法中较为常用，也能满足 Liao 的需求。但是编程实现稍稍有点麻烦，而且如果网格大小设计不合适，导致树的高度太高，每次查找需要遍历的路径太长，性能结果也比较差。
* GeoHash 算法
    * redis 的 GeoHash 并不会直接用经、纬度做 key，而是采用一种基于 Z 阶曲线的 编码方式，将二维的经、纬度，转化为一维的二进制数字，再进行 base32 编码
    * 得到两个二进制数后，再将它们合并成一个二进制数。合并规则是，从第一位开始，奇数位为经度，偶数位为纬度
    * 在 Redis 中，需要面对更通用的地理位置计算场景，所以 Redis 中的 GeoHash 并没有用 Hash 表存储，而是用跳表存储。
        * 所谓的 Z 阶曲线布局，本质其实就是基于 GeoHash 的二进制排序。将这些经过 编码的 2 进制数据用跳表存储。查找用户的时候，可以快速找到该用户，沿着跳表前后检 索，得到的就是邻近的用户。
### 10 | 搜索引擎设计:信息搜索怎么避免大海捞针?
Bingoo 的主要技术挑战包括:
1. 针对爬虫获取的海量数据，如何高效地进行数据管理;
2. 当用户输入搜索词的时候，如何快速查找包含搜索词的网页内容;
3. 如何对搜索结果的网页内容进行排序，使排在搜索结果列表前面的网页，正好是用户期 望看到的内容。
### 概要设计
一个完整的搜索引擎包括分布式爬虫、索引构造器、网页排名算法、搜索器等组成部分， Bingoo 的系统架构如下。
![](media/16773698371335/16773733353614.jpg)
索引的构造主要通过索引构造器完成，索引构造器读取 HDFS 中 的网页内容，解压缩后提取网页中的单词，构建一个“docID-> 单词列表”的正排索引。 然后，索引构造器再根据这个正排索引构建一个“单词 ->docID 列表”的倒排索引
设计了 64 个索引桶，根据 docID 取模，将不同网页分配到不同的桶中，在每个桶中 分别进行索引构建，通过并行计算来加快索引处理速度。
### 详细设计
#### 索引
首先，索引构造器将所有的网页都读取完，构建出所有的“docID-> 单词列表”正排索引。
然后遍历所有的正排索引，再按照“单词→docID 列表”的方式组织起来，就是倒排索引了。
拉链法
![](media/16773698371335/16773769767277.jpg)
跳表实际上是在链表上构建多级索引，在索引上遍历可以跳过底层的部分数据，我们可以 利用这个特性实现链表的跳跃式比较，加快计算速度。使用跳表的交集计算时间复杂度大 约是 O(log(n))。
#### PageRank 排名算法
对于 N 个网页，任何一个页面 Pi 的 PageRank 计算公式如下:
![](media/16773698371335/16773780263427.jpg) 公式中，Pj ∈ M (Pi ) 表示所有包含有 Pi 超链接的 Pj ，L(Pj ) 表示 Pj 页面包含的超链接 数，N 表示所有的网页总和。由于 Bingoo 要对全世界的网页进行排名，所以这里的 N 是 一个万亿级的数字。

## 11 | 反应式编程框架设计:如何使方法调用无阻塞等待?
反应式系统应该具备如下的 4 个特质。
* 即时响应
* 回弹性
    * 能够进行自我修复，保证正常 运行，保证响应，不会出现系统崩溃和宕机的情况。
* 弹性
    * 能够自动伸缩以适应应用负载压力，根据压 力自动调整自身的处理能力，或者根据自身的处理能力，调整进入系统中的访问请求数量。
* 消息驱动
    * 功能模块之间、服务之间通过消息进行驱动，以完成服务的流程。

目前主流的反应式编程框架有 RxJava、Reactor 等，它们的主要特点是基于观察者设计模式的异步编程方案，编程模型采用函数式编程。
反应式编程并不是必须用观察者模式和函数式编程。我们准备开发一个纯消息驱动，完全异步，支持命令式编程的反应式编程框架，框架名称为“Flower”。
### 需求分析
当并发用户请求到达应用服务器时，Web 容器线程不需要执行应用程序代码，它只是将用户的 HTTP 请求变为请求对象，将请求对象异步交给 Flower 框架的 Service 去处理，而 Web 容器线程自身立刻就返回。
### 概要设计
Flower 框架实现异步无阻塞，一方面是利用了 Java Web 容器的异步特性，主要是 Servlet3.0 以后提供的 AsyncContext，快速释放容器线程;另一方面则利用了异步的数据 库驱动和异步的网络通信，主要是 HttpAsyncClient 等异步通信组件。而 Flower 框架 内，核心应用代码之间的异步无阻塞调用，则是利用了 Akka 的 Actor 模型。
Akka Actor 的异步消息驱动实现如下。
![](media/16773698371335/16774169831386.jpg)
Flower 框架的主要元素包括:Flower Service(服务)、Flower 流程和 Flower 容器。其 中，Service 实现一个细粒度的服务功能，Service 之间会通过 Message 关联，前一个 Service 的返回值(Message)，必须是后一个 Service 的输入参数(Message)。而 Flower 容器就负责在 Service 间传递 Massage，从而使 Service 按照业务逻辑编辑成一 个 Flow(流程)。
### 详细设计
Flower核心类图如下
![](media/16773698371335/16774171165610.jpg)
Flower 框架核心关键类及其职责如下:
1. Service 以及 HttpService 接口是框架的编程核心，开发者开发的 Service 需要实现 Service 或者 HttpService 接口。HttpService 与 Service 的不同在于 HttpService 在接口方法中传递 Web 参数，开发者利用 Web 接口可以将计算结果直接 print 到 HTTP 客户端;
2. ServiceFactory 负责用户以及框架内置的 service 实例管理(加载 *.services 文件);
3. ServiceFlow 负责流程管理(加载 *.flow 文件);
4. ServiceActor 将 Service 封装到 Actor。

Flower 初始化及调用时序图如下。
![](media/16773698371335/16774175312566.jpg)
第一个过程是服务流程初始化过程。首先，开发者通过 ServiceFacade 调用已经定义好的服务流程。然后，ServiceFacade 根据传入的 flow 名和 service 名，创建第一个 ServiceActor。这个 ServiceActor 将通过 ServiceFactory 来装 载 Service 实例，并通过 ServiceFlow 获得当前 Service 在流程中所配置的后续 Service(可能有多个)。依此递归，创建后续 Service 的 ServiceActor，并记录其对应的 ActorRef。

第二个过程是消息流处理过程。调用者发送给 ServiceFacade 的消息，会被 flow 流程中的第一个 ServiceActor 处理，这个 ServiceActor 会调用对应的 Service 实 例，并将 Service 实例的返回值作为消息发送给流程定义的后续 ServiceActor。
#### 服务注册
开发者开发的服务需要在 Flower 中注册才可以调用，Flower 提供两种服务注册方式:配 置文件方式和编程方式。
#### 流程编排
通过流程编排方式，实现服务间依赖

## 12 | 高性能架构的三板斧:分析系统性能问题从哪里入手?
![](media/16773698371335/16774180067318.jpg)
其中，吞吐量 = 并发数 ÷ 响应时间
### 性能测试
* 性能测试
    * 这个过程中，随着并发数的增加，吞吐 量也在增加，但是响应时间变化不大。系统正常情况下的并发访问压力应该都在这个范围内。
* 负载测试
    * 这个过程中，随着并发数的增加，吞吐量只有小幅的增加，达到最大值后，吞吐量还会下降，而响应时间则会不断增加。
* 压力测试
    * 到了系统崩溃点，吞吐量为 0，响应时间无穷大。
![](media/16773698371335/16774183725482.jpg)

架构方面核心的优化思路有三个:**通过分布式集群扩展系统的服务器，降低单一服务器的负载压力;通过缓存的方式降低系统的读负载压力;通过消息队列降低系统的写负载压力**。对应的技术方案分别是:负载均衡、分布式缓存、消息队列，我称之为高性能架 构的三板斧。
### 负载均衡
**应用层负载均衡通常用在规模比较小的集群上**，而对于大规模的应用服务器集群， 我们使用 IP 层负载均衡或者链路层负载均衡。

IP 负载均衡不需要在 HTTP 协议层工作，可以在操作系统内核直接修改 IP 数据包的地址， 所以效率比应用层负载均衡高得多。

优化的方案就是采用**链路层负载均衡**。链路层负载均衡服务器并不修改请求数据包的 IP 地 址，而是修改数据链路层里的网卡 mac 地址，在数据链路层实现负载均衡。应用服务器返 回响应数据的时候，因为 IP 地址没有修改过，所以这个响应会直接到达用户的设备，而不 会再经过负载均衡服务器（目前大型互联网应用大多使用链路层负载均衡。）

### 分布式缓存
高并发架构中常见的分布式缓存有三种:CDN、反向代理和分布式对象缓存。

**CDN**(Content Delivery Network)即内容分发网络。目前很多互联网应用大约 80% 以上的网络流量都是通过 CDN 返回的。

**反向代理**则是 代理服务器，所有的网络请求都需要通过反向代理才能到达应用程序服务器。那么在这里 加一个缓存，尽快将数据返回给用户，而不是发送给应用服务器，这就是反向代理缓存。
一台服务器，既做反向代理服务器，也做负载均衡服务器。

CDN 和反向代理缓存对应用程序是透明的，通常被当做系统前端的一部分。而应用程序如 果要使用缓存，就需要**分布式对象缓存**。
![](media/16773698371335/16774193816399.jpg)
缓存只能改善系统的读操作性能，对于写操作，缓存是无能为力的。我们不能把用户提交的数据直接写入缓存中，因为缓存通常被认为是一种不可靠的存储。
### 消息队列
优化写操作性能的主要手段是使用消息队列，将写操作异步化。

一方面，用户请求发送给消息队列就可以直接返回响应给用户了，
而消息队列服务器的处理速度要远远快于数据库，用户端的响应时间可以极大缩短;
另一方面，消息队列写数据库的时候，可以根据数据库的负载能力控制写入的速度，即使用户请求并发很高，也不会导致数据库崩溃，消息队列可以使系统运行在一个性能最优的负载压力范围内。


-------------------------------------------------------------------
实践课三

## 13 | 微博系统设计:怎么应对热点事件的突发访问压力?
Weitter 的技术挑战，一方面是微博这样类似的信息流系统架构是如何设计的，另 一方面就是如何解决大 V 们的热点消息产生的突发高并发访问压力，保障系统的可用性
### 需求分析
Weitter 的核心功能只有三个:发微博，关注好友，刷微博。
1. 发微博:用户可以发表微博，内容包含不超过 140 个字的文本，可以包含图片和视频。
2. 关注好友:用户可以关注其他用户。
3. 刷微博:用户打开自己的微博主页，主页显示用户关注的好友最近发表的微博;用户向 下滑动页面(或者点刷新按钮)，主页将更新关注好友的最新微博，且最新的微博显示 在最上方;主页一次显示 20 条微博，当用户滑动到主页底部后，继续向上滑动，会按 照时间顺序，显示当前页面后续的 20 条微博。
4. 此外，用户还可以收藏、转发、评论微博。

#### 性能指标估算
系统按 10 亿用户设计，按 20% 日活估计，大约有 2 亿日活用户(DAU)，其中每个日活 用户每天发表一条微博，并且平均有 500 个关注者。
* 对于发微博所需的存储空间，我们做如下估算
    * 文本内容存储空间
    * 多媒体文件存储空间
* 对于刷微博的访问并发量，我们做如下估算
    * QPS（高峰期 QPS 按平均值 2 倍计算）
    * 网络带宽

### 概要设计
**系统架构的核心就是解决高并发的问题**，系统整体部署模型如下
![](media/16774596657002/16774600038003.jpg)
### 详细设计
#### 微博的发表 / 订阅问题
Weitter 最终采用“推拉结合”的模式。也就是说，如果用户当前在线，那么就会使用推模式，系统会在缓存中为其 创建一个好友最新发表微博列表，关注的好友如果有新发表微博，就立即将该微博插入列 表的头部，当该用户刷新微博的时候，只需要将这个列表返回即可。

如果用户当前不在线，那么系统就会将该列表删除。当用户登录刷新的时候，用拉模式为其重新构建列表。

那么如何确定一个用户是否在线?一方面可以通过用户操作时间间隔来判断，另一方面也可以通过机器学习，预测用户的上线时间，利用系统空闲时间，提前为其构建最新微博列表。
#### 缓存使用策略
LRU 算法并不适合微博的场景
我们在 Weitter 中使用时间淘汰算法，也就是将最近一定天数内发布的微博全部 缓存起来，用户刷新微博的时候，只需要在缓存中进行查找。如果查找到的微博数满足一 次返回的条数(20 条)，就直接返回给用户;如果缓存中的微博数不足，就再到数据库中查找。

对于特别热门的微博内容，针对单个微博内容的高并发访问，由于访问压力都集中一个缓存 key 上，会给单台 Redis 服务器造成极大的负载压力。因此，微博还会启用**本地缓存**模式，即应用服务器在内存中缓存特别热门的微博内容。
![](media/16774596657002/16774605165732.jpg)
#### 数据库分片策略
* 采用用户 ID 分片
    * 好处：单用户微博都在一台实例，查找方便
    * 缺点：明星大V或某用户频繁发微博，会导致这台服务器数据增长过快
* 采用微博 ID 分片
    * 可以避免上述按用户 ID 分片的热点聚集问题
    * 需要访问所有的分片数据库服务器才能得到所需的数 据，对数据库服务器集群的整体压力太大

最终，Weitter 采用按用户 ID 分片的策略。用户 ID 分片带来的热点问题，可以通过优化缓存来改善;而某个用户频繁发表 微博的问题，可以通过设置每天发表微博数上限(每个用户每天最多发表 50 条微博)来解决。
## 14 | 百科应用系统设计:机房被火烧了系统还能访问吗?
Wepedia 的功能比较简单，只有编辑词条和搜索查看词条这两个核心功能。设计目标主要是简单、高效地支持高并发访问，以及面对全球用户时保证 7 × 24 小时高可用。（支撑每日 10 亿次以上的访问压力）
### 概要设计
Wepedia 的整体架构，也就是简化的部署模型如图。
![](media/16774596657002/16774615020934.jpg)
在梳理 Wepedia 整体逻辑之前，先说明下架构图中核心组件的作用。
![](media/16774596657002/16774615349657.jpg)
具体的请求流程略。

Wepedia 为了解决 Nginx 缓存失效的问题，采用了另一种解决方案:失效通知。词条编 辑者修改词条后，Invalidation notification 模块就会通知所有 Nginx 服务器，该词条内 容失效，进而从缓存中删除它。

### 多数据中心架构
Wepedia 在全球部署多个数据中心，可以就近为用户提供服务。一是为了减少网络通信延迟，而是为了容灾备份。

Wepedia 的多数据中心架构如图。
![](media/16774596657002/16774620482401.jpg)
用户请求是 Get 请求(读请求)，那么请求就会在该数据中心处理。
如果请求是 Post 请求(写请求)，那么请求到达 Nginx 的时候，Nginx 会判断并将该 Post 请求转发给主数据中心。

通过这种方式，主数据中心根据 Post 请求更新数据库后，再通过 **Canal**组件将更新同步 给其他所有从数据中心的 MySQL，从而使所有数据中心的数据保持一致。同样， LightHttp 中的图片数据也进行同步，开发 LightHttp 插件，将收到的图片，发送给所有 从数据中心。

数据中心之间采用类似 ZooKeeper 的选主策略进行通信，如果主数据中心不可用，其他 数据中心会重新选举一个主数据中心。而如果某个从数据中心失火了，用户请求域名解析 到其他数据中心即可。

这种多数据中心架构虽然使词条编辑操作的时间变长，但是由于 Wepedia 的绝大多数请 求都是 Get 请求(Get 与 Post 请求比超过 1000:1)，因此对系统的整体影响并不很 大。同时用一种简单、廉价的方式实现多数据中心的数据一致性，开发和运维成本都比较低。
### 详细设计
详细设计重点关注 Wepedia 的性能优化。
#### 前端性能优化
前端是指应用服务器(也就是 PHP 服务器)之前的部分，包括 DNS 服务、 CDN 服务、 反向代理服务、静态资源服务等。

Wepedia 前端架构的核心是反向代理服务器 Nginx 集群，大约需要部署数十台服务器。 请求通过 LVS 负载均衡地分发到每台 Nginx 服务器，热点词条被缓存在这里，大量请求可 直接返回响应，减轻应用负载压力。而 Nginx 缓存 不能命中的请求，会再通过 LVS 发送 到 Apache 应用服务器集群。
在反向代理 Nginx 之前，是 CDN 服务。

Wepedia CDN 缓存的几条准则:
1. 内容页面不包含动态信息，以免页面内容缓存很快失效或者包含过时信息。
2. 每个内容页面有唯一的 REST 风格的 URL，以便 CDN 快速查找并避免重复缓存。
3. 在 HTML 响应头写入缓存控制信息，通过应用控制内容是否缓存及缓存有效期等。

#### 服务端性能优化
Wepedia 需要将最好的服务器部署在这里(和数据库配置一样的服务 器)，从硬件上改善性能。
除了硬件改善，Wepedia 还需要使用其他开源组件对应用层进行优化:
1. 使用 APC，这是一个 PHP 字节码缓存模块，可以加速代码执行，减少资源消耗。
2. 使用 Tex 进行文本格式化，特别是将科学公式内容转换成图片格式。
3. 替换 PHP 的字符串查找函数 strtr()，使用更优化的算法重构。
### 存储端性能优化
存储端优化最主要的手段是使用缓存，将热点数据缓存在分布式缓存系统的内存中。
Wepedia 的缓存使用策略如下:
1. 热点特别集中的数据直接缓存到应用服务器的本地内存中，因为要占用应用服务器的内 存且每台服务器都需要重复缓存这些数据，因此这些数据量很小，但是读取频率极高。
2. 缓存数据的内容尽量是应用服务器可以直接使用的格式，比如 HTML 格式，以减少应用 服务器从缓存中获取数据后解析构造数据的代价。
3. 使用缓存服务器存储 session 对象。

作为存储核心数据资产的 MySQL 数据库，需要做如下优化:
1. 使用较大的服务器内存。在 Wepedia 应用场景中，增加内存比增加其他资源更能改善 MySQL 性能。
2. 使用 RAID5 磁盘阵列以加速磁盘访问。
3. 使用 MySQL 主主复制及主从复制，保证数据库写入高可用，并将读负载分散在多台服务器。

## 15 | 限流器设计:如何避免超预期的高并发压力压垮系统?
我们准备开发一个限流器，产品名称为“Diana”。
### 需求分析
我们将 Diana 定位为一个限流器组件，即 Diana 的主要应用场景是部署在微服务网关或者 其他 HTTP 服务器入口，以过滤器的方式对请求进行过滤，对超过限流规则的请求返 回“服务不可用”HTTP 响应。
Diana 的限流规则可通过配置文件获取，并需要支持本地配置和远程配置两种方式，远程 配置优先于本地配置。限流方式包括:
*  全局限流:针对所有请求进行限流，即保证整个系统处理的请求总数满足限流配置。
*  账号限流:针对账号进行限流，即对单个账号发送的请求进行限流。
*  设备限流:针对设备进行限流，即对单个客户端设备发送的请求进行限流。
*  资源限流:针对某个资源(即某个 URL)进行限流，即保证访问该资源的请求总数满足 限流配置。

并且 Diana 设计应遵循开闭原则，能够支持灵活的限流规则功能扩展，即未来在不修改现 有代码和兼容现有配置文件的情况下，支持新的配置规则。
### 概要设计
Diana 的设计目标是一个限流器组件，即 Diana 并不是一个独立的系统，不可以独立部署 进行限流，而是部署在系统网关(或者其他 HTTP 服务器上)，作为网关的一个组件进行 限流，部署模型如下
![](media/16774596657002/16774724924693.jpg)
限流器的策略可以在本地配置，也可以通过远程的配置中心服务器加载，即远程配置。远程配置优先于本地配置。
#### 限流模式设计
本地记录称作本地限流，远程记录称作远程限流(也叫分布式限流)。

远程限流意味着，所有网关共享同一个限流数量。可能 某个网关服务器一段时间内根本就没有请求到达，但是远程的已处理请求数已经达到了限 流上限，那么这台网关服务器也必须拒绝请求。
我们使用 Redis 作为记录单位时间请求数 量的远程服务器。
#### 高可用设计
限流器应具有自动降级功能，即配置中心不可用，则使用本地配置;Redis 服务器不可用，则降级为本地限流。
### 详细设计
* 限流器运行期需要通过配置文件获取对哪些 URL 路径进行限流;
* 本地限流还是分布式限流;
* 对用户限流还是对设备限流，还是对所有请求限流;
* 限流的阈值是多少;
* 阈值的时间单位是什么；
* 具体使用哪种限流算法。
    * 常用的限流算法有 4 种，固定窗口(Window)限流算法，滑动窗口(Sliding Window) 限流算法，漏桶(Leaky Bucket)限流算法，令牌桶(Token Bucket)限流算法
#### 配置文件设计
Diana 限流器使用 YAML 进行配置。配置文件的配置项有 7 种，分别说明如下:
1. Url 记录限流的资源地址，"/“表示所有请求，配置文件中的路径可以互相包含，比 如“/”包含“/sample”，限流器要先匹配“/”的限流规则，如果“/”的限流规则还 没有触发(即访问”/"的流量，也就是单位时间所有的请求总和没有达到限流规则)， 则再匹配“/sample”。
2. 每个 Url 可以配置多个规则 rules，每个规则包括 actor，unit，rpu，algo，scope
3. actor 为限流对象，可以是账号(actor)，设备(device)，全部(all)
4. unit 为限流时间单位，可以是秒(second)，分(minute)，时(hour)，天 (day)
5. rpu 为单位时间限流请求数(request per unit)，即上面 unit 定义的单位时间内允许 通过的请求数目，如 unit 为 second，rpu 为 100，表示每秒允许通过 100 个请求，每 秒超过 100 个请求就进行限流，返回 503 响应
6. scope 为 rpu 生效范围，可以是本地(local)，也可以是全局(global)，scope 也 决定了单位时间请求数量是记录在本地还是远程，local 记录在本地，global 记录在远 程。
7. algo 限流算法，可以是 window，sliding window，leaky bucket，token bucket 。

* 固定窗口(Window)限流算法
    * 也是请求进入时才进行判断
* 滑动窗口(Sliding Window)限流算法
* 漏桶(Leaky Bucket)限流算法
    * 使用队列这种方式，实际上是把请求处理异步化了。因此 Diana 实现漏桶限流算法并不使用消息队列，而是阻塞等待。注意，以上代码多线程并发执行，需要进行加锁操作。
    * 使用漏桶限流算法，即使系统资源很空闲，多个请求同时到达时，漏桶也是慢慢地一个接一个地去处理请求，这其实并不符合人们的期望，因为这样就是在浪费计算资源。因此除非有特别的场景需求，否则不推荐使用该算法。
* 令牌桶(Token Bucket)限流算法
    * 令牌桶的实现，只需要在请求获取令牌的时候，通过时间计算，就可以算出令牌桶中的总令牌数。【总令牌数 = min(令牌数上限值，总令牌数 +(now - 最近生成令牌时间戳) / 令牌生成时间间隔)】
    * 建议通常场景中优先使用该算法，Diana 的缺省配置算法也是令牌桶。

## 16 | 高可用架构的十种武器：怎么度量系统的可用性？
我总结了一些高可用架构的技术方案，并称之为高可用架构的十种武器。
* 第一种武器：解耦
    * 组件的低耦合原则：
        * 无循环依赖原则：（即被依赖的组件尽量稳定，尽量少因为业务变化而变化）
        * 稳定依赖原则：（组件就要更加抽象）
        * 稳定抽象原则
    * 面向对象的低耦合原则
        * 开闭原则，即对修改封闭、对扩展开放；
    *    依赖倒置原则，即高层对象不能依赖低层对象，而是要依赖抽象接口，而抽象接口属于高层；
        * 接口隔离原则，不要强迫使用者依赖它们不需要的方法，要用接口对方法进行隔离
* 第二种武器：隔离
**隔离必须在低耦合的基础上进行才有意义**。如果组件之间的耦合关
系千头万绪、混乱不堪，隔离只会让这种混乱更雪上加霜。
* 第三种武器：异步
使用消息队列的异步架构，新用户注册消息发送给消息队列就立即返回，后续的操作通过
消费消息来完成，即使某个操作发生故障也不会影响用户注册成功。
* 第四种武器：备份
备份与失效转移（failover）总是成对出现的，共同构成一个高可用解决方案。
**最常见的备份就是负载均衡**。多台服务器构成一个集群，这些服务器天然就是互相备份的关系，任何一台服务器失效，只需要将分发到这台服务器的请求分发给其他服务器即可
* 第五种武器：重试
**可以重试的服务必须是幂等的**。所谓幂等，即服务重复调用和调用一次产生的结果
是相同的。
* 第六种武器：熔断
熔断的主要方式是使用断路器阻断对故障服务器的调用，断路器状态图如下。
* 第七种武器：补偿
补偿则是故障发生后，如何弥补错误或者避免损失扩大。
补偿最典型的使用场景是事务补偿。
传统的事务回滚主要依赖数据库的特性，当事务失败的时候，数据库执行自己的 undo 日
志，就可以将同一个事务的多条数据记录恢复到事务之初的状态。但是分布式服务没有
undo 日志，所以需要开发专门的事务补偿代码，当分布式事务失效的时候，调用事务补偿
服务，将事务状态恢复如初。
* 第八种武器：限流
* 第九种武器：降级
解决办法就是在系统高并发的时候（例如淘宝双十一），将确认收货、评价这些非核心的
功能关闭，也就是对系统进行降级，把宝贵的系统资源留下来，给正在购物的人，让他们
去完成交易。
* 第十种武器：多活
异地多活的架构需要考虑的重点是，用户请求如何分发到不同的机房去。这个主要可以在
域名解析的时候完成，也就是用户进行域名解析的时候，会根据就近原则或者其他一些策
略，完成用户请求的分发。另一个至关重要的技术点是，因为是多个机房都可以独立对外
提供服务，所以也就意味着每个机房都要有完整的数据记录。用户在任何一个机房完成的
数据操作，都必须同步传输给其他的机房，进行数据实时同步。
数据库实时同步最需要关注的就是数据冲突问题。同一条数据，同时在两个数据中心被修
改了，该如何解决？某些容易引起数据冲突的服务采用类似 MySQL 的主主模式，也就是
说多个机房在某个时刻是**有一个主机房**的，某些请求只能到达主机房才能被处理，其他的
机房不处理这一类请求，以此来避免关键数据的冲突。


除了以上的高可用架构方案，还有一些高可用的运维方案。
* 通过自动化测试减少系统的 Bug。
* 通过自动化监控尽早发现系统的故障。
* 通过预发布验证发现测试环境无法发现的 Bug。
* 通过灰度发布降低软件错误带来的影响。

## 17 | Web 应用防火墙：怎样拦截恶意用户的非法请求？
我们准备开发一个 Web 应用防火墙，该防火墙可作为 Web 插件，部署在 Web 应用或者微服务网关等 HTTP 服务的入口，拦截恶意请求，保护系统 安全。我们准备开发的 Web 应用防火墙名称为“Zhurong（祝融）”。
### 需求分析
HTTP 请求发送到 Web 服务器时，请求首先到达 Zhurong 防火墙，防火墙判断请求中是 否包含恶意攻击信息。如果包含，防火墙根据配置策略，可选择拒绝请求，返回 418 状态 码；也可以将请求中的恶意数据进行消毒处理，也就是对恶意数据进行替换，或者插入某 些字符，从而使请求数据不再具有攻击性，然后再调用应用程序处理
![](media/16774596657002/16777208640187.jpg)

### 概要设计
Zhurong 能够发现恶意攻击请求的主要手段，是对 HTTP 请求内容进行正则表达式匹配， 将各种攻击类型可能包含的恶意内容构造成正则表达式，然后对 HTTP 请求头和请求体进 行匹配。如果匹配成功，那么就触发相关的处理逻辑，直接拒绝请求；或者将请求中的恶 意内容进行消毒，即进行字符替换，使攻击无法生效。

其中，恶意内容正则表达式是通过远程配置来获取的。如果发现了新的攻击漏洞，远程配 置的漏洞攻击正则表达式就会进行更新，并在所有运行了 Zhurong 防火墙的服务器上生效，拦截新的攻击。组件图如下
![](media/16774596657002/16777208873040.jpg)


漏洞规则定义文件采用 XML 格式
* recipe 是漏洞定义文件的根标签，属性 attacktype 表示处理的攻击类型
* ruleSet 是漏洞处理规则集合，一个漏洞文件可以包含多个 ruleSet。stage 标签表示处理的阶段；condition 表示和其他规则的逻辑关系
* action 表示发现攻击后的处理动作。
* rule 表示漏洞规则，触发漏洞规则，就会引起 action 处理动作
### 详细设计
#### XSS 跨站点脚本攻击
XSS 攻击即跨站点脚本攻击 (Cross Site Script)，指黑客通过篡改网页，注入恶意 JavaScript 脚本，在用户浏览网页时，控制用户浏览器进行恶意操作的一种攻击方式。

常见的 XSS 攻击类型有两种，一种是反射型，攻击者诱使用户点击一个嵌入恶意脚本的链 接，达到攻击的目的。
另外一种 XSS 攻击是持久型 XSS 攻击，黑客提交含有恶意脚本的请求，保存在被攻击的 Web 站点的数据库中，用户浏览网页时，恶意脚本被包含在正常页面中，达到攻击的目的。


匹配成功后，根据漏洞定义文件，可以选择 forward 到错误页面，也可以采用 replace 方式进行消毒。
在 XSS 攻击字符前后加上“&nb sp;”（没空格)字符串，使得攻击脚本无法运行，同时在浏览器显 示的时候不会影响显示内容。
#### CSRF 跨站点请求伪造攻击
CSRF(Cross Site Request Forgery，跨站点请求伪造)，攻击者通过跨站请求，以合法用 户的身份进行非法操作，如转账交易、发表评论等

主要手法是利用跨站请求，在用户不知情的情况下，以用户的身份伪造请求。其核 心是利用了浏览器 Cookie 或服务器 Session 策略，盗取用户身份。

Zhurong 的防攻击策略，是过滤器自动在所有响应页面的表单 form 中添加一个隐藏字 段，合法用户在提交请求的时候，会将这个隐藏字段发送到服务器，防火墙检查隐藏字段 值是否正确
#### 注释与异常信息泄露

匹配 HTML 注释的正则表达式如下：
`“&lt;!--(.|&#x000A;|&#x000D;)*--&gt;”`
如果匹配到 HTML 注释，就用空字符串 replace 该注释。

## 18 | 加解密服务平台：如何让敏感数据存储与传输更安全？

我们设计了一个加解密服务系统，系统名称为“Venus”，统一管理所有的加解密 算法和密钥。应用程序只需要依赖加解密服务 SDK，调用接口进行加解密即可，而真正的 算法和密钥在系统服务端进行管理，保证算法和密钥的安全。

### 需求分析
一般说来，日常开发中的加解密程序存在如下问题：
1. 密钥（包括非对称加解密证书）保存在源文件或者配置文件中，存储分散而不安全。
2. 密钥没有分片交换机制，不能满足高安全级密钥管理和交换的要求。
3. 密钥缺乏版本管理，不能灵活升级，一旦修改密钥，此前加密的数据就可能无法解密。
4. 加密解密算法程序不统一，同样算法不同实现，内部系统之间密文不能正确解析。
5. 部分加解密算法程序使用了弱加解密算法和弱密钥，存在安全隐患。

Venus 是一个加解密服务系统，核心功能是加解密服务，辅助功能是密钥与算法管理。此外，Venus 还需要满足以下非功能需求：
* 安全性需求
    * 必须保证密钥的安全性，保证没有人能够有机会看到完整的密钥。因此一个密钥至少要 拆分成两片，分别存储在两个异构的、物理隔离的存储服务器中 。在需要进行密钥交换 的场景中，将密钥至少拆分成两个片段，每个管理密钥的人只能看到一个密钥片段，需 要双方所有人分别交接才能完成一次密钥交换。
* 可靠性需求
    * 加解密服务必须可靠，即保证高可用。无论在加解密服务系统服务器宕机、还是网络中 断等各种情况下，数据正常加解密都需要得到保障。
* 性能需求
    * 加解密计算的时间延迟主要花费在加解密算法上，也就是说，加载加解密算法程序、获 取加解密密钥的时间必须短到可以忽略不计。
系统用例图设计如下:
![](media/16774596657002/16777210093945.jpg)

系统主要参与者（Actor）包括：
![](media/16774596657002/16777210254284.jpg)

系统主要用例过程和功能包括：
1. 开发工程师使用密钥管理功能为自己开发的应用申请加解密算法和密钥；
2. 安全工程师使用密钥管理功能审核算法和密钥的强度是否满足数据安全要求；
3. （经过授权的）密钥管理者使用密钥管理功能可以查看密钥（的一个分片）；
4. 应用程序调用加解密功能完成数据的加密、解密；
5. 加密解密功能和密钥管理功能调用密钥服务功能完成密钥的存储和读取；
6. 密钥服务功能访问一个安全、可靠的密钥存储系统读写密钥。

总地说来，Venus 应满足如下需求：

1. 集中、分片密钥存储与管理，多存储备份，保证密钥安全易管理。
2. 密钥申请者、密钥管理者、密钥访问者，多角色多权限管理，保证密钥管理与传递的安 全。
3. 通过密钥管理控制台完成密钥申请、密钥管理、密钥访问控制等一系列密钥管理操作，实现便捷的密钥管理。
4. 统一加解密服务 API，简单接口，统一算法，为内部系统提供一致的加解密算法实现。
### 概要设计
针对上述加解密服务及密钥安全管理的需求，设计加解密服务系统 Venus 整体结构如下：
![](media/16774596657002/16777210436258.jpg)
#### 部署模型
Venus 部署模型如图：
![](media/16774596657002/16777210566481.jpg)

Venus 系统的核心服务器是 Key Server 服务器，提供密钥管理服务。密钥分片存储在文 件服务器 File Store 和数据库 DB 中。

#### 加解密调用时序图

加解密调用过程如下时序图所示。
![](media/16774596657002/16777211675678.jpg)

* 安全性：
    * 对密钥进行分片存储，不同存储服务器由不同运维人员管理。
* 性能：
    * 密钥缓存在 SDK 所在的进程，只有第一次调用时会访问远程的 Venus 服务器，加解密的性能只受加解密的数据大小和算法的影响
* 可靠性：
    * 密钥在缓存中，如果 Venus 服务器临时宕机，或者网络通信中断，也不会影响 到应用程序的正常使用
    * 服务器长时间宕机， 那么应用重新启动，本地缓存被清空，就需要重新请求密钥，这时候应用就不可用—>(解决方案)Venus 服务器、数据库和文件服务器做高可用备份。SDK 通过软负载均衡访问 Venus 服务器集群【失效转移、主从备份】

### 详细设计
#### 密钥领域模型
![](media/16774596657002/16777211987924.jpg)
#### 核心服务类设计
![](media/16774596657002/16777212714949.jpg)
以加密为例，具体处理过程时序图如下:
![](media/16774596657002/16777213439987.jpg)
#### 加解密数据接口 VenusData 设计
VenusData 用于表示 Venus 加解密操作输入和输出的数据，也就是说，加解密的时候构 造 VenusData 对象调用 Service 对应的方法，加解密完成后返回值还是一个 VenusData 对象。
![](media/16774596657002/16777214380418.jpg)
VenusData 用作输入时:
1. 属性 bytes 和 text 只要设置一个，即要么处理的是二进制 bytes 数据，要么是 Striing 数据，如果两个都设置了，Venus 会抛出异常。
2. 属性 version 可以不设置(即 null)，表示 Venus 操作使用的密钥版本是当前版本。
3. 属性 outputWithText 表示输出的 VenusData 是否处理为 text 类型，缺省值是 true。
4. 属性 dataWithVersion 表示加密后的 VenusData 的 bytes 和 text 中是否包含使用密 钥的版本信息，这样在解密的时候可以不指定版本，缺省值是 false。

如果 dataWithVersion 设置为 true，即表示加密后密文内包含版本号，这种情况下， VenusService 需要在**密文头部**增加 3 个字节的版本号信息，其中头两个字节为固定的 magic code:0x5E、0x23，第三个字节为版本号(也就是说，密钥版本号只占用一个字 节，最多支持 256 个版本)。

VenusData 用作输出时，Venus 会设置属性 keyName(和输入时的值一样)、 version、 bytes、 outputWithText、dataWithVersion(和输入时的值一样)，并根据 输入的 outputWithText 决定是否设置 text 属性。


-------------------------------------------------------------------
实践课四
## 19 | 许可型区块链重构：无中心的区块链怎么做到可信任？

一般我们把对所有公众都开放访问的区块链叫做“公有链”，而把若干企业构建的仅供企 业间访问的区块链叫做“**联盟链**”，有时候也称作“**许可型区块链**”。

而在公有链领域，目前看来，生态最完整、开发者社区最活跃、去中心化应用最多的公有 链技术莫过于 Ethereum 以太坊。

以我们准备在以太坊的代码基础上，进行若干代码模块的重构与开发。开发一个基于以太坊的企业级分布式账本与智能合 约平台，即一个许可型区块链。这个许可型区块链产品名称为“Taireum”。
### 需求分析

所谓区块链（block chain），就是将不断产生的数据按时间序列分组成一个一个连续的数 据区块（block），然后利用单向散列加密算法，求取每个区块的 Hash 值，并且在每个区 块中记录前一个区块的 Hash 值，这些区块就通过 Hash 值串成一个链条，被形象地称为 区块链。如

相比于比特币，以太坊最大的技术特点是支持智能合约，它是一种存储在区块链上的程 序，由链上的计算机节点分布式运行，是一种去中心化的应用程序，也是区块链企业级应 用必需的技术要求。

是以太坊是一种公有链技术，并不适合用于企业级的场景，原因主要有三个
1. 在**准入机制**上，使用以太坊构建的区块链网络允许任何节点接入
2. 在**共识算法**上,以太坊使用工作量证明（PoW）的方式对区块打包进行算力证明.工作量证明需要花费巨大的计算资源进行算力证 明，造成算力的极大浪费，也影响了区块链的交易吞吐能力。
3. 在**区块链运维管理**上，以太坊作为公有链，节点之间通过 P2P 协议自动组网，无需运维管理。


Taireum 需要在以太坊的基础上进行如下重构：

1. 重构以太坊的 P2P 网络通信模块，使其需要进行安全验证，得到联盟许可才能加入新节点，进入当前联盟链网络。
2. 重构以太坊的共识算法。只有经过联盟成员认证授权的节点才能打包区块，打包节点按序轮流打包，无需算力证明。
3. 开发联盟共识控制台 CCC（Consortium Consensus Console），方便对联盟链进行运维管理，联盟链用户只需要在 web console 上就可以安装部署联盟链节点，投票选举新 的联盟成员和区块授权打包节点。
### 概要分析
使用 Taireum 部署的联盟链如图:
![](media/16775463456448/16777216216315.jpg)
Taireum 部署模型如下:

![](media/16775463456448/16777215958147.jpg)

1. Taireum 中每个联盟企业都是一个 Taireum 节点，都需要完整地部署 Taireum+CCC 控制台， Client 使用我们提供的 web3jPlus sdk 与 Geth 进行 RPC 通信。
2. Geth 是 Tairem 编译出来的区块链运行程序，里面包含重写的 Tai 共识算法，重构后的 P2P 网络模块，以及原始的以太坊代码。
3. 不同节点之间的 Geth 使用 P2P 网络进行通信。
### 详细分析
#### Taireum 联盟共识控制台

联盟链运维管理开发的 web 组件，企业可以非常方便地使 用联盟共识控制台来部署联盟链运行节点、管理联盟成员和授权节点打包区块。

每个企业 节点的联盟共识控制台彼此独立，互不感知。他们需要通过调用联盟共识智能合约，对联 盟管理事务进行协商，以达成共识。

联盟链创立者节点的联盟共识控制台第一次成功部署联盟共识智能合 约时，就把这个合约的地址发给共识算法模块。共识算法在封装区块头的时候，将合约地 址写入区块头的 miner 中。

#### Taireum 联盟新成员许可入网

联盟链需要**保证联盟内数据的隐私和安全**。
Taireum 重构了以太坊的 P2P 通信模块，只有在许可列表中的节点才允许和当前联盟成员 节点建立连接，其他的连接请求在通信模块就会被拒绝，以此保证联盟链的安全和私密性。

Taireum 联盟新成员许可入网流程：
1. 新成员下载 Taireum，启动联盟共识控制台，然后在联盟共识控制台启动 Taireum 节点，获得节点 enode url。
2. 将 enode url 及公司信息提交给当前联盟链某个成员，该成员通过联盟共识智能合约发起新成员入网申请。
3. 联盟其他成员通过智能合约对新成员入网申请进行投票，得票数符合约定后，新成员信 息被记入成员列表。
4. 新成员节点通过网络连接当前联盟链成员节点，当前成员节点 p2p 通信模块读取智能合 约成员列表信息，检查新成员节点 enode url 是否在成员列表中，如果在，就同意建立连接，新成员节点开始下载区块数据。
#### Taireum 授权打包区块
Taireum 根据联盟链的应用特点，放弃了以太坊 ethash 工作量证明算法。在借鉴 clique 共识算法的基础上，Taireum 重新开发了 Tai 共识算法引擎，对联盟投票选出的授权打包节点排序，轮流进行区块打包。

Tai 共识算法引擎执行过程如下：

1. 联盟成员通过联盟共识智能合约投票选举授权打包区块的节点（在合约创建的时候，创 建者即联盟链创始人默认拥有打包区块的权限）。
2. Tai 共识算法通过联盟共识控制台访问智能合约，获得授权打包区块的节点地址列表， 并排序。
3. 检查父区块头的 extraData，解密取出父区块的打包者签名，查看该签名是否在授权打 包节点地址列表里，如果不在就返回错误。
4. 根据当前区块的块高（block number），对授权打包区块的节点地址列表长度取模，根 据余数决定对当前区块进行打包的节点，如果计算出来的打包节点为当前节点，就进行 区块打包，并把区块头难度系数设为 2，如果非当前节点，随机等待一段时间后打包区 块，并把区块头难度系数设为 1。难度系数的目的是尽量使当前节点打包的区块被加入 区块链，同时又保证当前打包节点失效的情况下，其他节点也会完成区块打包的工作。

Taireum 源码：https://github.com/taireum/go-taireum

## 20 | 网约车系统设计:怎样设计一个日赚 5 亿的网约车系统?
中国目前网约车用户规模约 5 亿，我们准备开发一个可支撑目前全部中国用户使用车平台，应用名称为“Udi”。
### 需求分析
用例图如下
![](media/16775463456448/16775465436868.jpg)
Udi 平台预计注册乘客 5 亿，日活用户 5 千万，平均每个乘客 1.2 个订单，日订单量 6 千 万。平均客单价 30 元，平台每日总营收 18 亿元。平台和司机按 3:7 的比例进行分成， 那么平台每天可赚 5.4 亿元。
另外，平台预计注册司机 5 千万，日活司机 2 千万。
### 概要设计
需要开发两个 App 应用，一个是给乘客的，用来叫车;一个是给司机的，用来接单。Udi 整体架构如下图:
![](media/16775463456448/16775466373218.jpg)
### 详细设计
* 关注网约车平台一些独有的技术特点:长连接管理、派单算 法、距离计算。
* 讨论 Udi 的订单状态模型。(所有交易类应用都非常重要的一个模型)
#### 长连接管理
我们选择让司机 App 和 Udi 系统 直接通过 TCP 协议进行长连接。一旦建立了连接，连接通道就需要长期保持，**不管是司机 App 发送位 置信息给服务器，还是服务器推送派单信息给司机App，都需要使用这个特定的连接通道。**
司机端的 TCP 长连接需要进行专门管理，处理司机 App 和服务器的连接信息，具体架构如下图。
![](media/16775463456448/16775470391721.jpg)
处理长连接的核心是 TCP 管理服务器集群。司机 App 会在启动时通过负载均衡服务器， 与TCP 管理服务器集群通信，请求分配一个 TCP 长连接服务器。
长连接管理的主要时序图如下
![](media/16775463456448/16775473023092.jpg)
如果TCP服务器宕机，长连接丢失。司机 App 需要重新通过 HTTP 来请求 TCP 管理服务器为它分配新的 TCP 服务器。TCP 管理服务器收到请求后， 一方面返回新的 TCP 服务器的 IP 地址和通信端口，一方面需要从 Redis 中删除原有的<司机 ID, 服务器名>键值对。
#### 距离计算
Udi 就是直接使用 Redis 的 GeoHash 进行邻近计算。司机的位置信息实时更新到 Redis 中，并直接调用 Redis 的 GeoHash 命令 georadius 计算乘客的邻近司机。

但是 Redis 使用**跳表**存储 GeoHash，Udi 日活司机两千万，每 3 秒更新一次位置信息， 平均每秒就需要对跳表做将近 7 百万次的更新，如此高并发地在一个跳表上更新，是系统不能承受的。所以，我们需要将司机以及跳表的粒度拆得更小。

Udi 以城市作为地理位置的基本单位，也就是说，每个城市在 Redis 中建立一个 GeoHash 的 key，这样，一个**城市范围内**的司机存储在一个跳表中。对于北京这样的超级城市，还 可以更进一步，以城区作为 key，进一步降低跳表的大小和单个跳表上的并发量。
#### 派单算法
Redis 计算的是两个点之间的空间距离，但是司机在城市的路线未必是直线。
派单算法需要从 Redis 中获取多个邻近用户上车点的空闲司机，然 后通过地理系统来计算每个司机到达乘客上车点的时间，最后将订单分配给花费时间最少的司机。

我们需要将一批订单聚合在一起，统一进行派单，如下图:
![](media/16775463456448/16775476903974.jpg)
分单子系统收到用户的叫车订单后，不是直接发送给派单引擎进行派单，而是发给一个订
单聚合池，订单聚合池里有一些订单聚合桶。订单写完一个聚合桶，就把这个聚合桶内的全部订单推送给派单引擎，由派单引擎根据整体时间最小化原则进行派单。
> 这里更倾向于是司机的总时间最小，因为司机的时间就是平台的金钱

这里的“写完一个聚合桶”，有两种实现方式，一种是间隔一段时间算写完一个桶，一种 是达到一定数量算写完一个桶。最后 Udi 选择间隔 3 秒写一个桶。

派单的时候需要依赖地理系统进行路径规划。事实上，乘客到达时间 和金额预估、行驶过程导航、订单结算与投诉处理，都需要依赖地理系统。Udi 初期会使 用第三方地理系统进行路径规划，但是将来必须要建设自己的地理系统。
#### 订单状态模型
散乱的订单状态变化无法统一描述订单的完整生命周期，因此我们设计了订单状态模型，如下图
![](media/16775463456448/16775479887172.jpg)
订单状态模型可以帮助我们总览核心业务流程，在设计阶段，可以通过状态图发现业务流程不完备的地方，在开发阶段，可以帮助开发者确认流程实现是否有遗漏。

## 21 | 网约车系统重构:如何用 DDD 重构网约车系统设计?
### DDD 的一般方法
**领域驱动设计就是从领域出发，分析领域内模型及其关系， 进而设计软件系统的方法。**

如果我们说要对 C2C 电子商务这个领域进行建模设计，那么这个范围就太大了，不知 道该如何下手。所以通常的做法是把整个领域拆分成多个子域，比如用户、商品、订单、 库存、物流、发票等。强相关的多个子域组成一个限界上下文。
![](media/16775463456448/16775483340791.jpg)
不同的限界上下文，也就是不同的子系统或者模块之间会有各种的交互合作——>DDD 使用**上下文映射图**来完成。

在 DDD 中，领域模型对象也被称为实体。先通过业务分析，识别出实体对象，然后通过相关的业务逻辑，设计实体的属性和方法。而限界上下文和上下文映射图则是微服务设计的关键，通常在实践中，限界上下文被设计为微服务，而上下文映射图就是微服务之间的依赖关系。具体设计过程如下图:
![](media/16775463456448/16775484694940.jpg)
首先，领域专家和团队一起讨论分析业务领域，确认业务期望，将业务分解成若干个业务场景。然后，针对每个场景画 UML 活动图，活动图中包含泳道，通过高内聚原则对功能 逻辑不断调整，使功能和泳道之间的归属关系变得更加清晰合理。**这些泳道最终就是限界上下文，泳道内的功能就是将来微服务的功能边界，泳道之间的调用流程关系，就是将来 微服务之间的依赖关系，即上下文映射图。**

根据康威定律:组织架构决定系统架构，两个团队维护一个微服务，必然会将这个微服务搞成事 实上的两个微服务。所以，我们还需要根据团队特性、过往的工作职责、技能经验，重新 对泳道图进行调整，使其符合团队的职责划分，这时候才得到限界上下文。

在这个限界上下文基础上，考虑技术框架、非功能需求、服务重用性等因素，进一步进行调整，就得到最终的限界上下文设计，形成我们的微服务架构设计。

### Udi DDD 重构设计
首先分析我们的业务领域，通过头脑 / 事件风暴的形式，收集领域内的所有事件 / 命令， 并识别事件 / 命令的发起方即对应的实体。最后识别出来的实体以及相关活动如下表：
![](media/16775463456448/16776338789139.jpg)
基于核心实体模型，绘制实体关系图，如下:
![](media/16775463456448/16776339044120.jpg)
在实体间关系明确且完整的前提下，我们就可以针对各个业务场景，绘制场景活动图。活动图比较多，这里仅用拼车场景作为示例
![](media/16775463456448/16776341192898.jpg)

依据各种重要场景的活动图，参考团队职责范围，结合微服务重用性考虑及非功能需求，产生限界上下文如下表:
![](media/16775463456448/16776340414757.jpg)
针对每个限界上下文进一步设计其内部的聚合、聚合根、实体、值对象、功能边界。以订单限界上下文为例:
![](media/16775463456448/16776340859363.jpg)
上述订单实体的属性和功能如下表:
![](media/16775463456448/16776341394264.jpg)
最后，在实现层面，设计对应的微服务架构如下图:
![](media/16775463456448/16776341641039.jpg)
这是一个基于领域模型的分层架构，最下层为聚合根对象，组合实体与值对象，完成核心业务逻辑处理。上面一层为领域服务层，主要调用聚合根对象完成订单子域的业务，根据业务情况，也会在这一层和其他微服务通信，完成更复杂的、超出当前实体职责的业务，所以这一层也是一个聚合层。
再上面一层是应用服务层，将实体的功能封装成各种服务，供各种应用场景调用。而最上面是一个接口层，提供微服务调用接口。
### 小结
我们把使用 DDD 进行系统重构的过程分为以下六步:
1. 讨论当前系统存在的问题，发现问题背后的根源。比如:架构与代码混乱，需求迭代困 难，部署麻烦，bug 率逐渐升高;微服务边界不清晰，调用依赖关系复杂，团队职责混 乱。
2. 针对问题分析具体原因。比如:微服务 A 太庞大，微服务 B 和 C 职责不清，团队内业 务理解不一致，内部代码设计不良，硬编码和耦合太多。
3. 重新梳理业务流程，明确业务术语，进行 DDD 战略设计，具体又可以分为三步。
    a. 进行头脑风暴，分析业务现状和期望，构建领域语言;
    b. 画泳道活动图、结合团队特性设计限界上下文;
    c. 根据架构方案和非功能需求确定微服务设计。
4. 针对当前系统实现和 DDD 设计不匹配的地方，设计微服务重构方案。比如:哪些微服 务需要重新开发，哪些微服务的功能需要从 A 调整到 B，哪些微服务需要分拆。
5. DDD 技术验证。针对比较重要、问题比较多的微服务进行重构打样，设计聚合根、实 体、值对象，重构关键代码，验证设计是否合理以及团队能否驾驭 DDD。
6. 任务分解与持续重构。在尽量不影响业务迭代的前提下，按照重构方案，将重构开发和 业务迭代有机融合。

## 22 | 大数据平台设计:如何用数据为用户创造价值?
大数据技术
则要将这些海量的用户数据进行关联计算，因此，适用于高并发架构的各种分布式技术并不能解决大数据的问题。
### Udi 大数据平台设计
根据 Udi 大数据应用场景的需求，需要将手机 App 端数据、数据库订单和用户数据、操作 日志数据、网络爬虫爬取的竞争对手数据统一存储到大数据平台，并支持数据分析师、算法工程师提交各种 SQL 语句、机器学习算法进行大数据计算，并将计算结果存储或返回。 Udi 大数据平台架构如下图:(蓝色属于大数据平台的组件)
![](media/16775463456448/16776344464583.jpg)
#### 大数据采集与导入
Udi 大数据平台整体可分为三个部分，第一个部分是大数据采集与导入。这一部分又可以 分为 4 小个部分，App 端数据采集、系统日志导入、数据库导入、爬虫数据导入。

后端的应用上报服务器收到前端采集的数据后，发送给消息队列，SparkStreamin 从消息 队列中消费消息，对数据进行清洗、格式化等 ETL 处理，并将数据写入到 HDFS 存储中。

Flume 日志收集系统会将 Udi 后 端分布式集群中的日志收集起来，发送给 SparkStreaming 进行 ETL 处理，最后写入到 HDFS 中。而 MySQL 的数据则通过 Sqoop 数据同步系统直接导入到 HDFS 中。

为了更好地应对市场竞争，Udi 还会通过网络爬 虫从竞争对手的系统中爬取数据。(模拟为普通用户)
#### 大数据计算
Udi 大数据平台的第二个部分是大数据计算。写入到 HDFS 中的数据，一方面供数据分析 师进行统计分析，一方面供算法工程师进行机器学习。

数据分析师会通过两种方式分析数据。
一种是通过交互命令进行即席查询，通常是一些较为简单的 SQL。分析师提交 SQL 后，在一个准实时、可接受的时间内返回查询结果，这个 功能是通过 Impala 完成的。
另外一种是定时 SQL 统计分析，通常是一些报表类统计，这 些 SQL 一般比较复杂，需要关联多张表进行查询，耗时较长，通过 Hive 完成，每天夜间服务器空闲的时候定时执行。

算法工程师则开发各种 Spark 程序，基于 HDFS 中的数据，进行各种机器学习。

以上这些大数据计算组件，Hive、Spark、SparkStreaming、Impala 都部署在同一个大 数据集群中，通过 Yarn 进行资源管理和调度执行。每台服务器既是 HDFS 的 DataNode 数据存储服务器，也是 Yarn 的 NodeManager 节点管理服务器，还是 Impala 的 Impalad 执行服务器。通过 Yarn 的调度执行，这些服务器上既可以执行 SparkStreaming 的 ETL 任务，也可以执行 Spark 机器学习任务，而执行 Hive 命令的时候，这些机器上运 行的是 MapReduce 任务。

#### 数据导出与应用
需要用 Sqoop 将 HDFS 中的数据导出到 MySQL 中，然后通过数据分析查询控制台，以图表的方式查看数据。

机器学习的计算结果则是一些学习模型或者画像数据，将这些数据推送给推荐引擎，由 推荐引擎实时响应 Udi 系统的推荐请求。

### Udi 大数据派单引擎设计
我们将利用这些数据优化 Udi 派单引擎。根据用户画像、车辆画像、乘车偏好进行同类匹配。
#### 基于乘客分类的匹配
根据乘客的注册信息、App 端采集的乘客手机型号、手机内安装应用列表、常用上下车地 点等，我们可以将乘客分类，然后根据同类乘客的乘车偏好，预测乘客的偏好并进行匹配。
#### 基于车辆分类的匹配
使用推荐引擎对派单系统进行优化，为乘客分配更合适的车辆，前提是需要对用户和车辆 进行分类与画像，想要完成这部分工作，我们可以在大数据平台的 Spark 机器学习模块通 过聚类分析、分类算法、协同过滤算法，以及 Hive 统计分析模块进行数据处理，将分类后 的数据推送给派单引擎去使用。

