# 无监督学习算法

## 一、无监督学习算法简介
1. 无监督学习的应用主要是分为两类，分别是聚类和降维
2. 聚类：聚类（Clustering）旨在将数据集中的样本分成若干个簇，使得同一个簇内的对象彼此相似，不同簇间的对象差异较大。聚类是一种无监督学习算法，不需要预先标记数据的标签，完全依赖数据本身内在结构和特征来进行分组，最终簇所对应的概念语义需由使用者来把握和命名。
3. 聚类应用场景：
   - 市场细分：将消费者按购买习惯分组
   - 图像分割：将图像像素按颜色或纹理聚类
   - 异常检测：识别不属于任何主要簇的异常点
   - 生物信息：对基因表达数据进行分组
4. 降维：提取数据中的本质特征，降低数据的特征维度

## 二、工作原理
1. 聚类的工作原理——聚类的核心是“物以类聚”，具体通过以下步骤实现：
   - 定义相似性：选择一个度量标准（如欧氏距离，余弦相似度）来衡量对象之间的相似性或距离。
   - 分组：根据相似性将对象分配到不同的簇中。
   - 优化：通过迭代或直接计算，调整簇的划分，使簇内相似性最大化，簇间差异最大化。
2. 降维的工作原理——通过数学变化，最大限度地以更少的参数表示数据，保证数据的完整性

## <font color='yellow'>**三、关键点**</font>
1. 【聚类】K均值算法
   - K均值聚类（K-means）是基于样本集合划分的聚类方法，将样本集合划分为k个子集构成k个簇，将n个样本分到k个簇中，每个样本到其所属簇的中心的距离最小。
   - 是一种硬聚类方法
   - 算法内容
     - 定义距离：欧式距离or...
     - 损失函数：样本到其对应中心的距离之和
     - K均值聚类转化为最优化问题，求解组合优化使得损失函数最小
     - 求解该问题一般采取迭代方法逐渐逼近最优解
       - 初始化：随机选择k个样本点作为初始中心
       - 计算每个样本到各个中心的距离，选择最小的距离作为聚类结果
       - **计算聚类结果中每个簇中所有样本的均值，作为新的簇中心**
       - 使用新的簇中心重复上述过程，直到收敛或符合停止条件（例如划分不再改变）
   - K均值聚类特点【初始中心点和k的大小都不清楚，类似于超参数需要试出来】
     - K均值聚类的初始中心的选择会直接影响聚类结果，并且不适合非凸形状簇
     - K均值聚类需要事先指定簇个数k，而实际中最优的k值是不知道的，需要尝试使用不同的k值检验聚类结果质量，可以采用二分查找快速找到最优k值。
     - 聚类结果的质量可以用簇的平均直径来衡量，一般地，簇个数变小时平均直径会增加；簇个数变大超过某个值后平均直径会不变，而这个值正是最优的k值
2. 【聚类】层次聚类
   - 层次聚类（Hierarchical Clustering）假设簇之间存在层次结构，将样本聚到层次化的簇中。层次聚类有自下而上的聚合方法和自上而下的分裂方法。因为每个样本只属于一个簇
   - 层次聚类属于硬聚类
   - 层次聚类的分类：聚合聚类 和 分裂聚类
     - 【自下而上的聚类】聚合聚类：开始将每个样本各自分到一个簇，之后将相距最近的两个簇合并，如此往复直至满足停止条件（例如达到预设的簇的个数、每个簇只包含一个样本、簇内样本相似性达到某个阈值等）
     - 【自上而下的聚类】分裂聚类：开始将整个数据集视作一个整体，之后根据某种距离或相似性度量，选择一个现有的簇将其分裂成两个簇，使分裂后子簇内相似性高，子簇间差异大，如此往复直至满足停止条件
3. 【聚类】密度聚类
   - 密度聚类（Density-Based Clustering）假设聚类结构能通过样本分布的紧密程度确定。通常情况下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展簇以获得最终聚类效果
   - DBSCAN算法
     - 概念定义：对于给定数据集$D = \{x_1, x_2, \dots, x_m\}$
       - **ε-邻域**：对于 $x_i \in D$，其 ε-邻域包含样本集 $D$ 中与 $x_i$ 的距离不大于`ε`的样本
       - **核心对象**：若 $x_i$ 的 ε-邻域至少包含`MinPts`个对象，则 $x_i$ 是一个核心对象
       - **密度直达**：若 $x_j$ 位于 $x_i$ 的 ε-邻域中，且 $x_i$ 是核心对象，则称 $x_j$ 由 $x_i$ 密度直达
       - **密度可达**：对 $x_i$ 和 $x_j$，若存在样本序列 $p_1,p_2,\dots,p_n$，其中 $p_1=x_i$，$p_n=x_j$，且 $p_{i+1}$ 由 $p_i$ 密度直达，则称 $x_j$ 由 $x_i$ 密度可达
       - **密度相连**：对 $x_i$ 和 $x_j$，存在 $x_k$ 使得 $x_i$ 和 $x_j$ 均由 $x_k$ 密度可达，则称 $x_j$ 与 $x_i$ 密度相连
       - **噪声点**：不属于任何簇的点，既不是核心对象也不在核心对象邻域内
     - `ε`、`MinPts`都是超参数
     - 密度直达：在自己的领域中；密度可达：可以由一个核心对象连接两边；密度相连：存在中间人可以彼此相连。
     - 算法内容：DBSCAN将簇定义为由密度可达关系导出的最大密度相连样本集合。DBSCAN先根据邻域参数（ε、MinPts）找出所有核心对象，再以任一核心对象为出发点找出由其密度可达的样本生成一个簇，直到所有核心对象均被访问过为止。
4. 聚类模型的评估
   - 不像有监督学习知道标签结果可以对比，怎么评价聚类结果的好坏呢？
   - 轮廓系数：计算内聚和分离
     - 计算每个样本到同簇其他样本的平均距离（内聚度 $a_i$）和到最近其他簇样本的平均距离（分离度 $b_i$），综合评价聚类紧密度和分离度。
     $$s_i=\frac{b_i-a_i}{\max(a_i,b_i)} \in [-1,1]$$
     - $s_i$的值越接近 1，聚类效果越好。总体轮廓系数是所有$s_i$的平均值。
   - 簇内平方和：只计算内聚
     - 衡量簇内数据点到簇中心的总距离平方和，常用于 K-means
     $$WCSS = \sum_{k=1}^K \sum_{i \in C_k} \|x_i - \mu_k\|^2$$。其中 $\mu_k$ 是第 $k$ 个簇的中心。
   - 肘部法：簇内平方和容易导致非常大的k值，综合考虑了让k变小和计算内聚度的两个方向，发明了肘部法
     - 肘部法用于确定最佳簇数K，在使用K-means时非常常见，它通过绘制簇数K和某个聚类质量指标（通常是簇内平方和）的关系曲线，找到一个拐点或“肘部”，即增加簇数带来的收益显著减少的点，这个点通常被认为是最佳的K值。
     - 也就是找一个下降程度相对变化大的点，作为K值
   - CH指数：簇间和簇内分散度的比值，也称方差比准则
     $$CH = \frac{BCSS/(K-1)}{WCSS/(N-K)}$$
     $$BCSS = \sum_{k=1}^K n_k \|\mu_k - \mu\|^2$$
     $$WCSS = \sum_{k=1}^K \sum_{i \in C_k} \|x_i - \mu_k\|^2$$
     - BCSS：簇间平方和，$n_k$ 是第 $k$ 个簇的样本数，$\mu_k$ 为第 $k$ 个簇的中心，$\mu$ 是所有样本的中心
     - WCSS：簇内平方和。描述内聚度的
5. 【降维】奇异值分解（SVD）：一种矩阵因子分解方法，用于将矩阵分解为更简单的形式，从而揭示数据的内在结构和特性。通过保留最大的几个奇异值及其对应的奇异向量，可以近似重构原始矩阵，减少数据维度，同时保留主要信息
   - 矩阵的奇异值分解是指将一个非零的实矩阵 $A \in \mathbb{R}^{n \times p}$ 表示为三个矩阵的乘积（因子分解）的形式：$$A = U\Sigma V^T$$
     - $U$ 是 $n$ 阶正交矩阵：$UU^T = I$
     - $V$ 是 $p$ 阶正交矩阵：$VV^T = I$
     - $\Sigma$ 是由降序排列的非负的对角元素组成的 $n \times p$ 矩形对角阵：$$\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_p),\quad \sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p \ge 0,\quad p = \min(n,p)$$
     - $U\Sigma V^T$ 称为矩阵 $A$ 的奇异值分解，$\sigma_i$ 称为矩阵 $A$ 的奇异值，$U$ 的列向量称为左奇异向量，$V$ 的列向量称为右奇异向量。任一实矩阵一定存在奇异值分解，且奇异值分解不唯一。
   - 降维的实现可以通过保留$U$的前几列来实现，需要几个就保留几列，从而实现降维
6. 【降维】主成分分析（PCA）：一种常用的无监督学习方法，旨在找到数据中“最重要的方向”，即方差最大的方向，并用这些方向重新表达数据
   - 主成分分析可直观解释为对数据所在的原始坐标系进行旋转变换，将数据投影到新坐标系的坐标轴上，新坐标系的第一坐标轴、第二坐标轴等分别表示第一主成分、第二主成分等
   - 实现
     - 传统的主成分分析通过协方差矩阵或相关矩阵的特征值分解进行
       - 协方差矩阵描述变量之间的方差和协方差，适用于未标准化的数据
       - 相关矩阵描述变量之间的标准化相关性，适用于标准化的数据
       - 特征值分解的结果给出了主成分的方向（特征向量）和每个主成分的方差（特征值）
     - 现在常用的方法是通过奇异值分解（SVD）进行主成分分析
   - 一般来说，保留了95%（差一点的70%、80%也可以）的，可以认为是保留了大部分的主成分，保留率可以通过计算保留的特征向量占特征值总和的比例来计算

## 四、优缺点
1. 密度聚类：密度聚类能识别任意形状的簇，可以自动识别并排除噪声点。但ε、MinPts的选择对密度聚类结果影响较大，且密度聚类难以适应密度变化较大的数据集。

## 五、实战
1. K-means的API
   ```python
   kmeans = KMeans(n_clusters=3)
   # n_clusters: 指定 K 的值
   kmeans.fit(X)  # 训练
   kmeans.predict(X)  # 预测
   kmeans.fit_predict(X)  # 训练并预测
   ```
2. 聚类评估API
   ```python
   from sklearn.metrics import silhouette_score, calinski_harabasz_score
   # 打印评价指标
   print(kmeans.inertia_) # 簇内平方和
   print(silhouette_score(X, y_pred)) # 计算轮廓系数
   print(calinski_harabasz_score(X, y_pred)) # 计算CH指数
   ```
3. 奇异值分解API
   ```python
   import numpy as np
   A = np.array([[1, 1], [2, 2], [0, 0]])
   U, S, V = np.linalg.svd(A)
   print(U)
   print(S)
   print(V)
   ```
4. 奇异值分解API + 降维
   ```pyhton
   from sklearn.utils.extmath import randomized_svd
   # 传入降低到多少维
   U, S, V = randomized_svd(A, n_components=2)
   print(U)
   print(S)
   print(V)
   ```
5. PCA
   ```python
   from sklearn.decomposition import PCA
   X = np.random.randn(100, 3)
   pca = PCA(n_components=2)
   X_pca = pca.fit_transform(X)
   print(X_pca.shape)
   ```

参考资料：
1. 尚硅谷机器学习视频：https://www.bilibili.com/video/BV1BYe4z5E9z

