# NLP技术实战——文本表示

## 一、文本表示（分词 + 词表示）
1. 文本表示：将自然语言转化为计算机能够理解的数值形式，是绝大多数自然语言处理（NLP）任务的基础步骤
2. 文本表示的第一步通常是分词和词表构建（就是一个映射关系）
   - 分词：原始文本切分为若干具有独立语义的最小单元（即token）的过程
   - 词表：是由语料库构建出的、包含模型可识别token的集合，是一种双向映射
3. 文本表示的第二部通常是词表示，就是将一句话表示成一个向量
4. 基础概念
   - corpus：语料
   - token：词元
   - vocabulary：词汇表

## 二、分词
1. 英文分词基本分类：按照分词的粒度大小，可以分为词级分词(word-level)、字符级分词(character-level)、子词级分词(subword-level)
   - 词级分词：
     - 定义：将文本按词语进行切分；
     - 问题：实际应用中容易出现 OOV(Out Of Vocabulary，未登录词)问题。所谓OOV，是指在模型使用阶段，输入文本中出现了不在预先构建词表中的词语，比如一些网络热词等
   - 字符级分词：
     - 定义：是以单个字符为最小单位进行分词的方法，文本中的每一个字母、数字、标点甚至空格，都会被视作一个独立的token。
     - 问题：单个字符本身语义信息极弱，模型必须依赖更长的上下文来推断词义和结构，这显著增加了建模难度和训练成本。此外，输入序列也会变得更长，影响模型效率。
   - 子词级分词：
     - 定义：将词语切分为更小的单元——子词（subword），例如词根、前缀、后缀或常见词片段
     - 优势：与词级分词相比，子词分词可以显著缓解OOV问题；与字符级分词相比，它能更好地保留一定的语义结构
   - 目前子词级的分词是大模型最常用的手段
2. 英文子级分词算法
   - BPE（Byte Pair Encoding）：学习阶段学习的是合并规则（可以将多个相邻的字符转成一个组合），分词阶段也是在应用合并规则，直到没有新的合并规则可用
     - 应用模型：GPT、GPT-2、RoBERTa、BART 和 DeBERTa
     - 学习阶段：将语料中的词汇拆成单个字符，构建初始词表；迭代**统计**语料中出现频率最高的相邻字符对，将其加入词表；迭代进行直到规定的词表数量上限
     - 分词阶段：文本预分词（对于英文就是按照空格和标点区分），文本拆分成最小单元（单个字符），然后根据学习阶段获取到的词表进行拼接，获取到对应的词表
     - 案例：
     ```txt
     1. 原始词库： hug(10次) pug(5次) pun(12次) bun(4次) hugs(5次)
     2. 学习阶段
        2.1 拆分单个字符，构成初始词表 [h u g p n b s]
        2.2 hug = [h u g] | pug = [p u g] | pun = [p u n] | bun = [b u n] | hugs = [h u g s]
        2.3 统计相邻字符串出现的次数，hu = 15次，ug = 20次，因此分词器学习到的第一个合并规则是ug，词表更新[h u g p n b s ug]
        2.4 hug = [h ug] | pug = [p ug] | pun = [p u n] | bun = [b u n] | hugs = [h ug s]
        2.5 再次统计相邻字符串出现的次数，un = 16次，因此学习到的第二个合并规则是un，词表更新[h u g p n b s ug un]
        2.6 ...迭代直到词表长度达到标准
     3. 分词阶段
        3.1 待分词语句，bug mug
        3.2 预分词，对英文来说就是使用空格进行区分
        3.3 拆分成单个字符，bug = [b u g] | mug = [m u g]，其中m不在原始词表中，被用[UNK]替代
        3.4 应用学习到的合并规则，ug合并...
        3.5 持续迭代进行合并
     ```
     - 代码**简单**实现，见`/NLP/code/token_test_bpe.py`
   - WordPiece：【注意：Google没有开源该算法的实现，以下是最佳猜测】
     - 应用模型：DistilBERT，MobileBERT，Funnel Transformers 和 MPNET
     - 学习阶段：将语料中的词汇**按照特定规则**拆成单个字符，构建初始词表；按照公示计算得分，优先合并得分比较高的，并将其加入词表，迭代进行直到规定的词表数量上限
       - 公式：`得分 = （词对出现的频率）÷（第一个元素出现的频率 × 第二个元素出现的频率）`
       - 公式解析：wordPiece关注的是两个元素合并的概率，如果u和n经常一起出现，但是u和n单独出现的频率也很高，u和n可能并不会被优先合并
       - 对比：对比BPE单纯关注两个词的共同出现频率，WordPiece关注的是合并的概率
     - 分词阶段：文本预分词（对于英文就是按照空格和标点区分），文本拆分成最小单元（单个字符），然后根据学习阶段获取到的词表进行拼接，获取到对应的词表
     - 案例：
     ```txt
     1. 原始词库： hug(10次) pug(5次) pun(12次) bun(4次) hugs(5次)
     2. 学习阶段
        2.1 拆分为单个字符，hug = [h ##u ##g] | pug = [p ##u ##g] | pun = [p ##u ##n] | bun = [b ##u ##n] | hugs = [h ##u ##g ##s]
        2.2 按照公式计算分数，如果按照BPE计算频率，第一个合并的是ug；但是wordPiece计算后，分数最高的是("##g", "##s")，进行合并然后扩充词表
        2.3 ...迭代直到词表长度达到标准
     3. 分词阶段
        3.1 待分词语句，bug bum
        3.2 预分词
        3.3 拆分成单个字符，bug = [b ##u ##g] | bum = [b ##u ##m]，其中m不在原始词表中，bum整体将会被用[UNK]替代，而不是["b", "##u", "[UNK]"]
     ```
     - 与BPE的区别
       - 合并公式不同
       - 对OOV的单词处理不同，BPE会将没有表示的部分展示为[UNK]，如 bum = ["b", "##u", "[UNK]"]，但是wordPiece会将整体变为[UNK]，如 bum = ["[UNK]"]
   - Unigram Language Model：
     - 应用模型：AlBERT，T5，mBART，Big Bird 和 XLNet
     - 学习阶段：算法获取语料中所有的字符串组合形式，然后计算剔除哪个组合的损失最小，直到获得规定的词库大小，从而达到获得分词的效果
     - 分词阶段：文本预分词（对于英文就是按照空格和标点区分），文本拆分成最小单元（单个字符），然后根据学习阶段获取到的词表进行拼接，获取到对应的词表
     - 案例：
     ```txt
     1. 原始词库： hug(10次) pug(5次) pun(12次) bun(4次) hugs(5次)
     2. 学习阶段
        2.1 获取所有的字符串子串 ["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
     3. 分词阶段
        3.1 执行剔除逻辑，看看怎么分词，能够最好地表达一个词语
        3.2 为了对一个给定的单词进行分词，我们会查看所有可能的分词组合，并根据 Unigram 模型计算出每种可能的概率，例如 pug = [p u g] | hug = [pu g]
        3.3 计算表示概率：用p+u+g表示pug的概率 = p出现的概率 * u出现的概率 * g出现的概率，用pu+g表示pug的概率 = pu出现的概率 * g出现的概率
        3.4 计算结果如下：["p", "u", "g"] : 0.000389 | ["p", "ug"] : 0.0022676 | ["pu", "g"] : 0.0022676
        3.5 pug将采用["p", "ug"]或者["pu", "g"]来标识
     ```
3. 中文分词基本类型
   - 字符级分词：按照汉字进行切分
   - 词级分词：将中文文本按照词语进行切分，切分结果贴近人类表达习惯；中文没有空格等边界，词级分词往往依赖词典、规则和模型等来识别词语边界
   - 子词级分词：虽然中文没有英文中的子词结构，但是BPE算法可以直接使用在英文中，原理是一样的；当前主流中文大模型都是这种方案
4. 中文分词基本算法，与英文分词的基本算法一致，包括BPE、WordPiece、Unigram等

## 三、分词工具
1. 分词工具分类
   - 【规则获取】基于词典或者模型的传统方案，主要是以词为单位进行切分（更死板）
     - jieba：https://github.com/fxsjy/jieba
     - HanLP：https://github.com/hankcs/HanLP
   - 【统计获取】基于子词建模算法的方案，从数据中学习词表（更智能化），实际上是基于统计获得的
     - Hugging Face Tokenizer
     - SentencePiece
     - tiktoken
2. 分词工具实战
   - jieba
     ```txt
     import jieba
  
     words_generator = jieba.cut("我是一个大学生，我在上课")
  
     # 返回一个生成器
     for word in words_generator:
         print(word)
  
     # 返回一个列表
     words_list = jieba.lcut("我是一个大学生，我在上课")
     print(words_list)
     ```
   - jieba支持自定义词典

## 四、文本表示
1. 词表示：为了让模型能够理解和处理文本，必须将这些 token 转换为计算机可以识别和操作的数值形式
2. 表示方法：one-hot、语义化词向量Word2Vec模型、上下文相关词表示
3. 语义化词向量：通过对大规模语料的学习，为每个词生成一个具有语义意义的稠密向量表示。这些向量能够在连续空间中表达词与词之间的关系，使得“意思相近”的词在空间中距离更近
   - Word2Vec模型：基于分布假设，一个词的含义由它周围的词决定
     - CBOW：continuous bag-of-words模式，输入一个词的上下文，模型的目标是预测中间的目标词
       ```txt
       我读书 -- 我 读 书
       相当于用 我 和 书 来标识 读
       ```
     - skip-gram：输入是一个中心词，模型的目标是预测其上下文中的所有词（前后的若干个词）
       ```txt
       我读书 -- 我 读 书
       相当于用 读 来标识 我 和 读
       ```
     - 通过这种方式，语义相近的（也就是上下文类似的），会有更近的空间距离从而获得相似的标识
   - 获取词向量，可以使用公开的，也可以自己用上面的算法训练
     - 使用公开的，可从[网站](https://github.com/Embedding/Chinese-Word-Vectors)下载
     ```python
     from gensim.models import KeyedVectors

     model_path = 'sgns.renmin.word.bz2'
     model = KeyedVectors.load_word2vec_format(model_path)
     si = model.similarity('地铁', '图书馆')
     print(si)
     0.27362388
     ```
     - 自己训练，也都是使用现成的库
     ```python
     import jieba as ooo
     from gensim.models import Word2Vec, KeyedVectors
     import pandas as pd
     
     df = pd.read_csv('online_shopping_10_cats.csv', encoding='utf-8').dropna()
     
     sentences = [
         [token for token in ooo.lcut(review) if token.strip() != '']
         for review in df["review"]
     ]
     model = Word2Vec(
         sentences,  # 已分词的句子序列
         vector_size=100,  # 词向量维度
         window=5,  # 上下文窗口大小
         min_count=2,  # 最小词频（低于将被忽略）
         sg=1,  # 1 = Skip-Gram，0 = CBOW
         workers=4  # 并行训练线程数
     )
     # 将训练好的模型保存
     model.wv.save_word2vec_format('my_vectors.kv')
     
     # 重新加载模型
     my_model = KeyedVectors.load_word2vec_format('my_vectors.kv')
     print(my_model['地铁'])
     # 剩下的场景，这个model就可以直接使用了
     ```
4. 应用词向量
   
5. 



参考资料：
1. 分词体验：https://tiktokenizer.vercel.app/
2. BPE分词算法：https://hf-mirror.com/learn/llm-course/zh-CN/chapter6/5
3. WordPiece分词算法：https://huggingface.co/learn/llm-course/zh-CN/chapter6/6
4. Unigram分词算法：https://huggingface.co/learn/llm-course/zh-CN/chapter6/7
5. 