# 用PyTorch进行深度学习

## 一、激活函数
1. PyTorch中已经实现了神经网络中可能用到的各种激活函数，我们在代码中只要直接调用即可
2. 实战使用
   ```python
   import torch
   
   sigmoid_value = torch.sigmoid(x)
   tanh_value = torch.tanh(x)
   relu_value = torch.relu(x)
   
   # 定义标准正态分布
   m = torch.randn(3, 5)
   # 需要定义dim=1，代表每一行是一个分类概率
   softmax_value = torch.softmax(m, dim=1)
   ```

## 二、参数初始化和正则化
1. 全连接层：在神经网络中，参数主要位于全连接层（仿射层Affine）中。PyTorch提供了`torch.nn`模块，专门用于神经网络的构建和训练。其中全连接层被实现为`Linear`类，内部有两个属性：权重`weight`和偏置`bias`；这就是神经网络的主要参数。
   ```python
   import torch.nn as nn
   # 定义了一个有5个输入神经元、2个输出神经元的全连接层
   linear = nn.Linear(5, 2)
   ```
2. 常数初始化：所有权重参数初始化为一个常数。【不推荐使用】
   ```python
   import torch.nn as nn

   linear = nn.Linear(5, 2)
   
   # 全部参数初始化为0
   nn.init.zeros_(linear.weight)
   print(linear.weight)
   
   # 全部参数初始化为1
   nn.init.ones_(linear.weight)
   print(linear.weight)
   # Parameter containing:
   # tensor([[0., 0., 0., 0., 0.],
   #        [0., 0., 0., 0., 0.]], requires_grad=True)
   # 注意：打印出来的是一个 5*2 的矩阵，这主要是因为pytorch在底层实现上为了反向传播计算方便 
   # y = W.T @ X + b，其中pytorch真实记录的是W.T
   
   # 全部参数初始化为一个常数
   nn.init.constant_(linear.weight, 10)
   print(linear.weight)
   ```
3. 秩初始化：权重参数初始化为单位矩阵
   ```python
   import torch.nn as nn

   linear = nn.Linear(5, 2)
   
   # 参数初始化为单位矩阵
   nn.init.eye_(linear.weight)
   print(linear.weight)
   ```
4. 正态分布初始化：权重参数按指定均值与标准差正态分布初始化
   ```python
   import torch.nn as nn

   linear = nn.Linear(5, 2)
   
   # 参数初始化为按指定均值与标准差正态分布
   nn.init.normal_(linear.weight, mean=0.0, std=1.0)
   print(linear.weight)
   ```
5. 均匀分布初始化：权重参数在指定区间内均匀分布初始化
   ```python
   import torch.nn as nn

   linear = nn.Linear(5, 2)
   
   # 参数初始化为在区间内均匀分布
   nn.init.uniform_(linear.weight, a=0, b=10)
   print(linear.weight)
   ```
6. Xavier初始化（Glorot初始化）：Xavier初始化根据输入和输出的神经元数量调整权重的初始范围，确保每一层的输出方差与输入方差相近。适用于<font color='yellow'>`Sigmoid`和`Tanh`</font>等激活函数，能有效缓解梯度消失或爆炸问题
   ```python
   import torch.nn as nn

   linear = nn.Linear(5, 2)
   
   # Xavier正态分布初始化
   nn.init.xavier_normal_(linear.weight)
   print(linear.weight)
   
   # Xavier均匀分布初始化
   nn.init.xavier_uniform_(linear.weight)
   print(linear.weight)
   ```
7. He初始化（Kaiming初始化）：He初始化根据输入的神经元数量调整权重的初始范围。主要适用于<font color='yellow'>`ReLU`及其变体（如`Leaky ReLU`）</font>激活函数。
   ```python
   import torch.nn as nn

   linear = nn.Linear(5, 2)
   
   # Kaiming正态分布初始化
   nn.init.kaiming_normal_(linear.weight)
   print(linear.weight)
   
   # Kaiming均匀分布初始化
   nn.init.kaiming_uniform_(linear.weight)
   print(linear.weight)
   ```
8. Dropout随机失活：一种在学习的过程中随机关闭神经元的方法
   ```python
   import torch

   # 创建一维张量
   x = torch.randint(1, 10, (10,), dtype=torch.float32)
   # 定义一个Dropout层
   dropout = nn.Dropout(p=0.5)
   
   y = dropout(x)
   
   print(x)
   # tensor([1., 1., 8., 4., 9., 4., 5., 4., 4., 1.])
   print(y)
   # tensor([ 0.,  2.,  0.,  8., 18.,  0.,  0.,  8.,  8.,  0.])
   ```

## 三、搭建神经网络
1. 模块：在神经网络框架中，由多个层组成的组件称之为模块（Module）。
   - 在PyTorch中模型就是一个Module，各网络层、模块也是Module。Module是所有神经网络的基类。
   - 在定义一个Module时，我们需要继承torch.nn.Module并主要实现两个方法：
     - `__init__`：定义网络各层的结构，并初始化参数。
     - `forward`：根据输入进行前向传播，并返回输出。计算其输出关于输入的梯度，可通过其反向传播函数进行访问（通常自动发生）。forward方法是每次调用的具体实现。
2. 模型的输入层一般不计入层数计算，一般来说层数是指抛除输入层后的剩余层数
   ```text
   为什么输出层最后一层经常不特殊关注参数初始化？
   1. 输出层承担的作用不同：
      神经网络的隐藏层承担特征提取的核心作用，糟糕的初始化（比如权重过大 / 过小）会导致梯度消失 / 爆炸、训练停滞，因此需要 Xavier、He 等专用初始化策略；
      而输出层只是把隐藏层提取的特征映射到任务目标（分类 / 回归），它的参数只需要完成 “最后一步映射”，即使初始值是随机小值，反向传播也能快速修正，不会影响整体训练流程。
   2. 输出层参数的训练容错率高
      隐藏层参数的初始化错误会层层传递、放大，导致整个网络训练失败；而输出层是最后一层，即使初始值略有偏差，隐藏层已经提取了有效特征，输出层参数只需要少量迭代就能调整到合理范围。
   3. 不特殊初始化不代表不初始化
      PyTorch中Linear层默认用kaiming_uniform_初始化权重，偏置初始化为0
   4. 输出层的激活函数对参数的敏感程度较低
      输出层的激活函数Softmax、Identity对参数初始值的依赖程度不高
   ```
3. 实现搭建一：使用Module
   - 模块架构：
     - 输入层：三个特征 + 偏置
     - 隐藏层1：使用Xavier正态分布初始化权重，激活函数使用Tanh
     - 隐藏层2：使用He正态分布初始化权重，激活函数使用ReLU
     - 输出层：按默认方式初始化，激活函数使用Softmax
   - 手写model，实现代码
   ```python
   import torch
   from torch import nn
   
   # 定义神经网络Module
   class MyNet(nn.Module):
   
       # 初始化网络结构
       def __init__(self):
           # 父类初始化
           super(MyNet, self).__init__()
   
           # 第一层神经网络，定义为3*4的结构 + 偏置
           self.layer1 = nn.Linear(in_features=3, out_features=4)
           # 使用xavier初始化，初始化网络参数，因为后面的激活函数是tanh
           nn.init.xavier_uniform_(self.layer1.weight)
   
           # 第二层神经网络，定义为4*4的结构 + 偏置
           self.layer2 = nn.Linear(in_features=4, out_features=4)
           # 使用kaiming初始化，初始化网络参数，因为后面的激活函数是relu
           nn.init.kaiming_normal_(self.layer2.weight)
   
           # 第三层输出层
           self.layer3 = nn.Linear(in_features=4, out_features=2)
           # 输出层通常不特别初始化
   
       # 定义前向传播方法
       def forward(self, x):
           x = self.layer1(x)
           x = torch.tanh(x)
   
           x = self.layer2(x)
           x = torch.relu(x)
   
           x = self.layer3(x)
           x = torch.softmax(x, dim=1)
           return x
   
   # 测试模型定义
   # 1. 定义输入数据
   x = torch.randn(10, 3)
   # 2. 定义模型
   model = MyNet()
   # 3. 前向传播
   output = model.forward(x)
   print(output)
   ```
4. 查看模型参数
   - 逐个查看
     ```python
     # 1. 逐个查看所有参数
     print(model.linear1.weight)
     print(model.linear1.bias)
     print(model.linear2.weight)
     print(model.linear2.bias)
     print(model.out.weight)
     print(model.out.bias)
     ```
   - 调用parameters查看
     ```python
     for param in model.parameters():
         print(param)
     
     for name, param in model.named_parameters():
         print(name, param)
     ```
   - 获取字典
     ```python
     print(model.state_dict())
     ```
   - 查看模型结构和参数数量
     ```python
     # 4. 查看模型的架构和参数数量
     from torchsummary import summary
     # input_size表明输入的数据结构
     summary(model, input_size=(3, ), batch_size=10, device=device)
     ```
5. 实现搭建二：Sequential
   ```python
   import torch
   from torch import nn
   from torchsummary import summary
   
   # 1. 准备数据
   x = torch.randn(10,3)
   
   # 2. 创建网络
   myNet = nn.Sequential(
      nn.Linear(in_features=3, out_features=4),
      nn.Tanh(),
      nn.Linear(in_features=4, out_features=4),
      nn.ReLU(),
      nn.Linear(in_features=4, out_features=2),
      nn.Softmax(dim=1),
   )
   
   # 3. 定义一个参数初始化的函数
   def init_param(layer):
      if isinstance(layer, nn.Linear):
          nn.init.xavier_uniform_(layer.weight)
          nn.init.constant_(layer.bias, 0.1)
   
   # 4. 参数初始化
   myNet.apply(init_param)
   
   # 5. 前向传播
   output = myNet(x)
   print(output)

   # 6. 查看参数
   summary(myNet, input_size=(3,), batch_size=10, device='cpu')
   ```

## 四、损失函数
1. 分类任务损失函数
   - 二分类问题损失函数
     ```python
     import torch
     import torch.nn as nn
     
     # 输入数据
     input = torch.randn(3, 2)
     print(input)
     # 得到预测概率值
     pred = torch.sigmoid(input)
     print(pred)
     
     # 目标值
     target = torch.tensor([[0,1],[1,0],[0,1]], dtype=torch.float)
     print(target)
     
     # 定义损失函数
     loss = nn.BCELoss()
     
     # 计算损失函数
     loss_value = loss(pred, target)
     ```
   - 多分类问题损失函数
     ```python
     import torch
     import torch.nn as nn
     
     # 1. 目标值为顺序编码的标签
     input = torch.randn(6, 8)
     print(input)
     
     # 目标值，6个数据的类别标签（0~7）
     target = torch.tensor([1, 0, 3, 7, 5, 2])
     print(target)
     
     # 定义损失函数
     loss = nn.CrossEntropyLoss()
     loss_value = loss(input, target)
     
     # 2. 目标值为一组概率（独热编码）
     input = torch.randn(6, 8)
     print(input)
     
     target = torch.randn(6, 8).softmax(dim=1)
     print(target)
     
     loss = nn.CrossEntropyLoss()
     loss_value = loss(input, target)
     ```
2. 回归任务损失函数
   - MSE、L2 Loss
     ```python
     # MSE: L2 Loss
     mse_loss = nn.MSELoss()
     loss_value = mse_loss(input, target)
     ```
   - MAE、L1 Loss
     ```python
     # MAE: L1 Loss
     mae_loss = nn.L1Loss()
     loss_value = mae_loss(input, target)
     ```
   - Smooth L1
     ```python
     # Smooth L1
     smooth_l1_loss = nn.SmoothL1Loss()
     loss_value = smooth_l1_loss(input, target)
     ```




## 五、参数更新方法


## 六、应用案例：房价预测




参考资料：
1. 尚硅谷深度学习视频：https://www.bilibili.com/video/BV1MRJmzSEaa

