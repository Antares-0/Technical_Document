# 决策树算法

## 一、决策树算法简介
1. 决策树（Decision Tree）是一种基于树形结构的算法，根据一系列条件判断逐步划分数据，缩小范围，最终得出预测结果。
2. 决策树由四部分组成：
   - 根节点：树的节点，包含所有数据。
   - 内部节点：表示特征上的判断条件。
   - 分支：根据判断条件分出的路径。
   - 叶节点：最终分类或回归的结果。
3. **决策树适用于需要规则化、可解释性和快速决策的场景，尤其在数据特征明确、样本量适中的情况下表现良好**。在复杂任务中，它常作为基础模型，与集成学习结合（如随机森林、梯度提升树）以提升性能

## 二、工作原理
1. 决策树的学习通常包括3个步骤：特征选择、决策树的生成和决策树的剪枝
2. 特征选择：递归地选择最优特征，并根据该特征对训练数据进行划分，使得对各个子数据集有一个最好的分类
3. 决策树生成：
   - 首先构建根结点，将所有训练数据都放在根结点
   - 选择一个最优特征，按照这一特征将训练数据集划分成子集，使得各个子集有一个在当前条件下最好的分类
   - 如果这些子集已经能够被**基本正确分类**（防止过拟合），那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行划分并构建相应的结点
   - 如此递归直至所有训练数据子集被基本正确分类，或者没有合适的特征为止
4. 决策树的剪枝：可能发生过拟合现象。**因此需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点或更高的结点，然后将父结点或更高的结点改为新的叶结点**
5. 决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优

## <font color='yellow'>**三、关键点**</font>
1. 信息熵、条件熵、信息增益、信息增益率、基尼指数
   - 信息熵：表示随机变量不确定性的度量，熵只依赖于`X`的分布，与`X`的取值无关。熵越大，随机变量的不确定性就越大。
     - 设`X`是一个取有限个值的离散随机变量，其概率分布为：
       $$P(X=x_i) = p_i,\quad i=1,2,\dots,n$$
     - 随机变量`X`的熵定义为：
       $$H(X) = -\sum_{i=1}^n p_i \log p_i$$
     - `n=2`时，`X`的信息熵为：
       $$H(X) = -p\log p - (1-p)\log(1-p)$$，`p = 1`和`p = 0`的时候事情是确定的，因此信息熵都是0
   - 条件熵：表示在已知随机变量𝑋的条件下随机变量𝑌的不确定性
     - 条件熵`(H(Y|X)`表示在已知随机变量`X`的条件下随机变量`Y`的不确定性：$H(Y|X) = \sum_{i=1}^n P(X=x_i)\, H(Y|X=x_i)$
     - 条件熵越小，可以说特征选的越好
   - 信息增益：表示由于特征`A`而使得对数据集`D`的分类的不确定性减少的程度
     - 给定训练集数据`D`和特征`A`，熵`H(D)`表示对数据集进行分类的不确定性，条件熵`H(D|A)`表示在特征`A`给定条件下对数据集`D`进行分类的不确定性，两者之差即信息增益：
       $$g(D,A) = H(D) - H(D|A)$$
     - 信息增益大的特征具有更强的分类能力。
     - 每次进行特征选择时，计算数据集中每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征
       - 计算数据集`D`的熵`H(D)`：
         $$H(D) = -\sum_{k=1}^K \frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$$
       - 计算特征`A`对数据集`D`的条件熵`H(D|A)`：
         $$H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$$
       - 计算信息增益
         $g(D,A) = H(D) - H(D|A)$
   - 信息增益率
     - 为什么会出现信息增益率的概念？使用信息增益划分训练数据集的特征，会倾向于选择取值较多的特征，会导致分的类型太多（分的越细，内部的聚合程度越高，信息熵越低）
     - 使用信息增益率可以对上述问题进行校正，这是特征选择的另一个准则
     - $g_R(D,A) = \frac{g(D,A)}{H_A(D)}$，其中 $H_A(D) = -\sum_{i=1}^n \frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$，$H_A(D)$会因为分类太多而变大，导致分母变大从而实现惩罚的效果
   - 基尼指数
     - 有K个类，样本属于第k类的概率为$p_k$，则概率分布的基尼指数：$$Gini(p) = \sum_{k=1}^K p_k (1-p_k) = 1 - \sum_{k=1}^K p_k^2$$
     - 对于给定的样本集合D，其基尼指数：$$Gini(D) = 1 - \sum_{k=1}^K \left( \frac{|C_k|}{|D|} \right)^2$$
     - 如果样本集合`D`根据特征`A`是否取某一可能值`a`被划分为两个子集$D_1,D_2$，则在特征`A`的条件下，集合`D`的基尼指数：$$Gini(D,A) = \frac{|D_1|}{|D|}\,Gini(D_1) + \frac{|D_2|}{|D|}\,Gini(D_2)$$
     - 分布越均衡，基尼指数越大；分类越多，基尼指数越大；
     - 基尼指数越小，表示确定性越强；基尼指数越大，表示越不确定
2. 怎么选出最优分类的特征？通常特征选择的准则是信息增益或信息增益率
   - ID3算法（依赖信息增益）：在决策树各个节点上应用**信息增益**选择特征，递归地构建决策树
     - 从根节点开始，计算所有可能的信息增益，选择信息增益最大的特征作为节点特征，由该特征的不同取值建立子节点
     - 再依次对子节点进行上述操作，直到所有特征信息增益均很小或无特征可选为止，最终生成决策树
   - C4.5算法（依赖信息增益率）：在决策树各个节点上应用**信息增益率**选择特征，递归地构建决策树
   - CART算法（依赖基尼指数）：在决策树各个节点上应用**基尼指数**选择特征，递归地构建决策树
     - 算法步骤
       - CART决策树是一棵二叉树，根据基尼指数生成决策树，对训练数据集`D`的每个特征`A`的每一个可能的取值`a`，计算`A=a`时的基尼指数，选择基尼指数**最小的特征及其对应的切分点**作为最优特征与最优切分点，并生成两个子节点，将训练数据集依特征分配到两个子节点中
       - 重复上述过程，直到节点中样本数小于阈值、或样本集的基尼指数小于阈值、或没有更多特征
     - 算法优势：基尼指数的计算不依赖于离散或连续，因此该算法既可以用于回归也可以用于分类
     - 需要计算最优切分特征和最优切分点
3. 决策树的剪枝
   - 为什么需要剪枝？避免过拟合，简化模型
   - 决策树的剪枝通常分为预剪枝和后剪枝
     - 预剪枝：提前设置限制条件，停止树的成长；但是过于严格的停止条件会导致欠拟合
       - 限制最大树深度
       - 限制每个节点最小样本数
       - 限制最小的误差减小量
       - 限制最大叶节点数量
     - 后剪枝：生成完树后，再减少枝干
       - 代价复杂度剪枝（CCP）：类似正则化，剪枝or枝条太多会惩罚
       - 减少误差剪枝（REP）：直接验证，剪枝是否会提高准确率？

## 四、实战

参考资料：
1. 尚硅谷机器学习视频：https://www.bilibili.com/video/BV1BYe4z5E9z

