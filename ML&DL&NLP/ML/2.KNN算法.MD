# KNN算法

## 一、KNN算法简介
1. 算法名：K近邻算法
2. 分类：有监督学习算法
3. 使用方式：可以用于分类，也可以用于回归
   - 分类：离散数据场景
   - 回归：连续数据场景
4. K近邻算法（K-Nearest Neighbors，KNN）核心思想是通过计算给定样本与数据集中所有样本的距离，找到距离最近的K个样本，然后根据这K个样本的类别或值来预测当前样本的类别或值

## 二、工作原理
1. 计算距离：计算分类样本与训练集中每个样本之间的距离
2. 选择N个近邻：选择其中N个最近的邻居，根据他们的分类投票决定当前输入的分类
3. 引入权重：可以引入距离权重，在附近k个点“一半一半”的时候，可以通过距离进一步确定

## <font color='yellow'>**三、关键点**</font>
1. 计算距离的方法
   - 为什么需要关注计算距离的方法：距离的计算方法是k近邻的核心，决定了k个近邻分别是谁
   - 有哪些计算距离的方法
      1. 欧式距离：欧几里得距离（勾股定理），也就是L2范数
      2. 曼哈顿距离：坐标轴上的绝对距离之和（街区距离），例如`在街区中从一点移动到另一点的场景`
      3. 切比雪夫距离：坐标轴上的绝对轴距最大值，例如`国际象棋中计算王移动次数的场景`
      4. 闵可夫斯基距离：多维空间中的标准计算方式，上述距离都可以看作是闵可夫斯基距离的特殊情况
         - 定义：每个维度距离之间的差值p次方求和后开p次根号（也就是Lp范数）
         - 曼哈顿距离：p = 1；欧式距离：p = 2；切比雪夫距离：p = ∞
2. 最近邻居数量K：是一个超参数
   - 为什么需要邻居数量：邻居数量的大小决定了模型的拟合程度
      - K越大，越容易导致欠拟合；想象每次都是根据全部样本的平均来决定当前样本的分类
      - K越小，越容易导致过拟合；想象每次都获取最近的一个样本的分类来决定当前样本的分类
3. 归一化和标准化操作
   - 为什么需要关注归一化和标准化操作：数据之间的量纲不一致，需要将数据按照比例缩放到一个固定范围，使得距离的计算变得均衡
   - 关于归一化：
     - 什么时候使用归一化操作？模型对输入范围敏感的时候，可以优先考虑归一化；归一化不改变原始分布形状，但对异常值比较敏感。**当数据分布有明显边界（如图像像素值、文本词频），或模型对输入范围敏感时可以优先考虑归一化**
     - 归一化有哪些好处？
       - 消除量纲差异：不同特征的单位或量纲可能差异巨大（例如身高以米为单位，体重以千克为单位），归一化可消除这种差异，避免模型被大范围特征主导
       - 加速模型收敛：对于梯度下降等优化算法，归一化后特征处于相近的尺度，优化路径更平滑，收敛速度更快
       - 适配特定模型需求：某些模型（如神经网络、K近邻、SVM）对输入数据的范围敏感，归一化能显著提升其性能
     - 归一化方法：将数据缩放到`[0, 1]`之间，`Xnew = (x - Xmin) / (Xmax - Xmin)`
     - 归一化实战
       ```python
       from sklearn.preprocessing import MinMaxScaler
       X = [[2, 1], [3, 1], [1, 4], [2, 6]]
       # 归一化，区间设置为(-1,1)
       X = MinMaxScaler(feature_range=(-1, 1)).fit_transform(X)
       print(X)
       ```
   - 关于标准化：
     - 什么时候使用标准化操作？大多数情况下标准化更通用，尤其是数据分布或者存在轻微异常值的时候
     - 标准化有哪些好处？
       - 适应数据分布：将数据转换为均值为0、标准差为1的分布，适合假设数据服从正态分布的模型（如线性回归、逻辑回归）。 
       - 稳定模型训练：标准化后的数据对异常值的敏感度较低（相比归一化），鲁棒性更强。
       - 统一特征尺度：与归一化类似，标准化也能消除量纲差异，但更关注数据的统计分布而非固定范围。
     - 标准化方法：将数据调整为均值为0、标准差为1的标准分布，`Xnew = (x - x(平均值)) / x(标准差)`
     - 标准化实战
       ```python
       from sklearn.preprocessing import StandardScaler
       X = [[2, 1], [3, 1], [1, 4], [2, 6]]
       # 标准化
       X = StandardScaler().fit_transform(X)
       print(X)
       ```
4. 模型保存与加载（dump 与 load）
   - 大型模型使用joblib，joblib比pickle更快，使用了多线程并行处理。
     ```python
     # 保存模型
     joblib.dump(value=knn, filename="knn_model")
     # 加载模型，对新数据进行预测
     knn_loaded = joblib.load("knn_model")
     y_pred = knn_loaded.predict(X_test[10:11])
     print(f"预测类别：{y_pred}, 真实类别：{y_test[10]}")
     ```
   - 小型模型使用pickle：pickle是python自带的保存模型的方法。
     ```python
     from sklearn import svm
     from sklearn import datasets
     import pickle
     
     clf = svm.SVC()  # 创建一个模型
     iris = datasets.load_iris()
     X, y = iris.data, iris.target
     clf.fit(X, y)  # 训练模型
     
     # 保存模型
     with open('save/clf.pickle', 'wb') as f:
        pickle.dump(clf,f)

     # 加载模型
     with open('save/clf.pickle', 'rb') as f:
        clf2 = pickle.load(f)
     print(clf2.predict(X[0:1]) # 加载了模型之后就可以进行预测了 输出[0]
     ```
5. 网格搜索与超参数调整
   - 网格搜索：网格搜索（Grid Search）是一种系统化的超参数调优方法，通过遍历预定义的超参数组合，找到使模型性能最优的参数配置。通过自动化调参避免手动试错，提高效率
   - 网格搜索通常嵌套交叉验证，与交叉验证结合以提高调参的可靠性
     - 外层循环：遍历参数网格中的每个参数组合
     - 内层循环：对每个参数组合使用交叉验证评估模型性能

## 四、优缺点
1. 优点：简单直观、易于理解和实现
2. 缺点：计算量大、对噪声数据比较敏感

## 五、实战
1. 分类任务demo，使用到的是分类器
   ```python
   from sklearn.neighbors import KNeighborsClassifier
   import numpy as np
   
   # 准备数据
   X = np.array([[2, 1], [3, 1], [1, 4], [2, 6]])
   y = np.array([0, 1, 0, 1])    # 分类标签
   
   # 定义KNN分类模型，使用距离作为权重，当附近K个点存在一半一半的情况时，由距离决定
   knn = KNeighborsClassifier(n_neighbors=2, weights='distance')
   
   # 模型训练
   knn.fit(X, y)
   
   # 预测
   x = np.array([[4, 9]])
   x_class = knn.predict(x)
   print(x_class)
   ```
2. 回归任务demo，使用到的是回归器
   ```python
   from sklearn.neighbors import KNeighborsRegressor
   # 准备数据
   X = [[2, 1], [3, 1], [1, 4], [2, 6]]
   y = [0.5, 0.33, 4, 3]    # 分类标签
   
   # KNN回归模型
   knn = KNeighborsRegressor(n_neighbors=2, weights='distance')
   knn.fit(X, y)
   # 预测
   x = [[4, 9]]
   x_pred = knn.predict(x)
   print(x_pred)
   ```
3. 心脏病预测case：见`(./code&data/knn/heart_disease.py)`代码实战
   1. 数据集
   2. 特征工程
      - 数据集中包含多种类型的特征
        - 类别型（独热编码处理）：如果直接使用整数的类别编码会被算法视为有序数值，导致错误的距离计算，使用独热编码解决问题
          - 胸痛类型：4种分类（名义变量）
          - 静息心电图结果：3种分类（名义变量）
          - 峰值ST段的斜率：3种分类（有序变量）
          - 地中海贫血：4种分类（名义变量）
        - 数值型（标准化处理）：年龄、静息血压、胆固醇、最大心率、运动后的ST下降、主血管数量
        - 二元特征（保持原样）：性别、空腹血糖、运动性心绞痛
      - 避免多重共线性：特征之间存在高度线性相关关系的现象，通过在代码中指明`OneHotEncoder(drop="first")`来降低特征维度，加快模型的训练速度
        - 核心思想就是降维，因为多个特征之间存在必然的线性关系，没必要使用n个特征来表示信息，使用(n-线性相关数)来代表就可以了
   3. 模型训练与评估
      - 超参数的调整：交叉验证&网格搜索
   4. 模型保存

参考资料：
1. 尚硅谷机器学习视频：https://www.bilibili.com/video/BV1BYe4z5E9z
2. 模型保存与加载：https://blog.csdn.net/vincent_duan/article/details/121276754