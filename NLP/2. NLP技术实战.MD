# NLP技术实战——文本表示

## 一、文本表示（分词 + 词表示）
1. 文本表示：将自然语言转化为计算机能够理解的数值形式，是绝大多数自然语言处理（NLP）任务的基础步骤
2. 文本表示的第一步通常是分词和词表构建（就是一个映射关系）
   - 分词：原始文本切分为若干具有独立语义的最小单元（即token）的过程
   - 词表：是由语料库构建出的、包含模型可识别token的集合，是一种双向映射
3. 文本表示的第二部通常是词表示，就是将一句话表示成一个向量
4. 基础概念
   - corpus：语料
   - token：词元
   - vocabulary：词汇表

## 二、分词
1. 英文分词基本分类：按照分词的粒度大小，可以分为词级分词(word-level)、字符级分词(character-level)、子词级分词(subword-level)
   - 词级分词：
     - 定义：将文本按词语进行切分；
     - 问题：实际应用中容易出现 OOV(Out Of Vocabulary，未登录词)问题。所谓OOV，是指在模型使用阶段，输入文本中出现了不在预先构建词表中的词语，比如一些网络热词等
   - 字符级分词：
     - 定义：是以单个字符为最小单位进行分词的方法，文本中的每一个字母、数字、标点甚至空格，都会被视作一个独立的token。
     - 问题：单个字符本身语义信息极弱，模型必须依赖更长的上下文来推断词义和结构，这显著增加了建模难度和训练成本。此外，输入序列也会变得更长，影响模型效率。
   - 子词级分词：
     - 定义：将词语切分为更小的单元——子词（subword），例如词根、前缀、后缀或常见词片段
     - 优势：与词级分词相比，子词分词可以显著缓解OOV问题；与字符级分词相比，它能更好地保留一定的语义结构
   - 目前子词级的分词是大模型最常用的手段
2. 英文子级分词算法
   - BPE（Byte Pair Encoding）：学习阶段学习的是合并规则（可以将多个相邻的字符转成一个组合），分词阶段也是在应用合并规则，直到没有新的合并规则可用
     - 应用模型：GPT、GPT-2、RoBERTa、BART 和 DeBERTa
     - 学习阶段：将语料中的词汇拆成单个字符，构建初始词表；迭代**统计**语料中出现频率最高的相邻字符对，将其加入词表；迭代进行直到规定的词表数量上限
     - 分词阶段：文本预分词（对于英文就是按照空格和标点区分），文本拆分成最小单元（单个字符），然后根据学习阶段获取到的词表进行拼接，获取到对应的词表
     - 案例：
     ```txt
     1. 原始词库： hug(10次) pug(5次) pun(12次) bun(4次) hugs(5次)
     2. 学习阶段
        2.1 拆分单个字符，构成初始词表 [h u g p n b s]
        2.2 hug = [h u g] | pug = [p u g] | pun = [p u n] | bun = [b u n] | hugs = [h u g s]
        2.3 统计相邻字符串出现的次数，hu = 15次，ug = 20次，因此分词器学习到的第一个合并规则是ug，词表更新[h u g p n b s ug]
        2.4 hug = [h ug] | pug = [p ug] | pun = [p u n] | bun = [b u n] | hugs = [h ug s]
        2.5 再次统计相邻字符串出现的次数，un = 16次，因此学习到的第二个合并规则是un，词表更新[h u g p n b s ug un]
        2.6 ...迭代直到词表长度达到标准
     3. 分词阶段
        3.1 待分词语句，bug mug
        3.2 预分词，对英文来说就是使用空格进行区分
        3.3 拆分成单个字符，bug = [b u g] | mug = [m u g]，其中m不在原始词表中，被用[UNK]替代
        3.4 应用学习到的合并规则，ug合并...
        3.5 持续迭代进行合并
     ```
     - 代码**简单**实现，见`/NLP/code/token_test_bpe.py`
   - WordPiece：【注意：Google没有开源该算法的实现，以下是最佳猜测】
     - 应用模型：DistilBERT，MobileBERT，Funnel Transformers 和 MPNET
     - 学习阶段：将语料中的词汇**按照特定规则**拆成单个字符，构建初始词表；按照公示计算得分，优先合并得分比较高的，并将其加入词表，迭代进行直到规定的词表数量上限
       - 公式：`得分 = （词对出现的频率）÷（第一个元素出现的频率 × 第二个元素出现的频率）`
       - 公式解析：wordPiece关注的是两个元素合并的概率，如果u和n经常一起出现，但是u和n单独出现的频率也很高，u和n可能并不会被优先合并
       - 对比：对比BPE单纯关注两个词的共同出现频率，WordPiece关注的是合并的概率
     - 分词阶段：文本预分词（对于英文就是按照空格和标点区分），文本拆分成最小单元（单个字符），然后根据学习阶段获取到的词表进行拼接，获取到对应的词表
     - 案例：
     ```txt
     1. 原始词库： hug(10次) pug(5次) pun(12次) bun(4次) hugs(5次)
     2. 学习阶段
        2.1 拆分为单个字符，hug = [h ##u ##g] | pug = [p ##u ##g] | pun = [p ##u ##n] | bun = [b ##u ##n] | hugs = [h ##u ##g ##s]
        2.2 按照公式计算分数，如果按照BPE计算频率，第一个合并的是ug；但是wordPiece计算后，分数最高的是("##g", "##s")，进行合并然后扩充词表
        2.3 ...迭代直到词表长度达到标准
     3. 分词阶段
        3.1 待分词语句，bug bum
        3.2 预分词
        3.3 拆分成单个字符，bug = [b ##u ##g] | bum = [b ##u ##m]，其中m不在原始词表中，bum整体将会被用[UNK]替代，而不是["b", "##u", "[UNK]"]
     ```
     - 与BPE的区别
       - 合并公式不同
       - 对OOV的单词处理不同，BPE会将没有表示的部分展示为[UNK]，如 bum = ["b", "##u", "[UNK]"]，但是wordPiece会将整体变为[UNK]，如 bum = ["[UNK]"]
   - Unigram Language Model：
     - 应用模型：AlBERT，T5，mBART，Big Bird 和 XLNet
     - 学习阶段：算法获取语料中所有的字符串组合形式，然后计算剔除哪个组合的损失最小，直到获得规定的词库大小，从而达到获得分词的效果
     - 分词阶段：文本预分词（对于英文就是按照空格和标点区分），文本拆分成最小单元（单个字符），然后根据学习阶段获取到的词表进行拼接，获取到对应的词表
     - 案例：
     ```txt
     1. 原始词库： hug(10次) pug(5次) pun(12次) bun(4次) hugs(5次)
     2. 学习阶段
        2.1 获取所有的字符串子串 ["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
     3. 分词阶段
        3.1 执行剔除逻辑，看看怎么分词，能够最好地表达一个词语
        3.2 为了对一个给定的单词进行分词，我们会查看所有可能的分词组合，并根据 Unigram 模型计算出每种可能的概率，例如 pug = [p u g] | hug = [pu g]
        3.3 计算表示概率：用p+u+g表示pug的概率 = p出现的概率 * u出现的概率 * g出现的概率，用pu+g表示pug的概率 = pu出现的概率 * g出现的概率
        3.4 计算结果如下：["p", "u", "g"] : 0.000389 | ["p", "ug"] : 0.0022676 | ["pu", "g"] : 0.0022676
        3.5 pug将采用["p", "ug"]或者["pu", "g"]来标识
     ```
3. 中文分词基本类型
   - 字符级分词
   - 词级分词
   - 子词级分词
4. 中文



参考资料：
1. 分词体验：https://tiktokenizer.vercel.app/
2. BPE分词算法：https://hf-mirror.com/learn/llm-course/zh-CN/chapter6/5
3. WordPiece分词算法：https://huggingface.co/learn/llm-course/zh-CN/chapter6/6
4. Unigram分词算法：https://huggingface.co/learn/llm-course/zh-CN/chapter6/7