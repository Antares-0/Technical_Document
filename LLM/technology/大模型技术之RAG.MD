# 大模型技术之RAG

## 一、什么是RAG
1. RAG就是通过检索获取相关的知识并将其融入Prompt，让大模型能够参考相应的知识从而给出合理回答。因此，可以将RAG的核心理解为“检索+生成”。
   - 前者主要是利用向量数据库的高效存储和检索能力，召回目标知识
   - 后者则是利用大模型和Prompt工程，将召回的知识合理利用，生成目标答案
   - 用户向LLM问问题，RAG从各种数据源获取相关信息，将检索得到的结果融合到prompt中，由LLM综合分析获得答案
2. RAG的完整流程
   - 数据准备阶段：主要是将私域数据向量化后构建索引并存入数据库的过程
     - 数据提取
       - 数据加载：从不同源头获得多种数据格式的数据，依赖数据自身的情况统一处理为一种格式
       - 数据处理：数据过滤、压缩、格式化等
       - 元数据提取：提取数据中的关键信息
     - 文本分割：文本分割需要考虑两个条件，语义和Tokens的长度限制，普遍来说有以下两种分割方式
       - 固定长度分割：根据embedding模型的token长度限制，将文本分割为固定长度，会损失掉大部分的语义信息；可以通过句分割 + 冗余信息来缓解
       - 句分割：按照句子切分，常见的切分符号包括：感叹号、问号、换行符等
     - 向量化embedding：向量化是一个将文本数据转换为向量矩阵的过程，该过程会直接影响到后续检索的效果
       - 常见的向量化模型如下：
         
         |    Model Name     | description |                                  url                                   |
         |:-----------------:|:-----------:|:----------------------------------------------------------------------:|
         | ChatGPT-Embedding | OpenAI公司提供  | https://platform.openai.com/docs/guides/embeddings/what-are-embeddings |
         |  ERNIE-Embedding  |   百度公司提供    |         https://cloud.baidu.com/doc/WENXINWORKSHOP/s/alj562vvu         |
         |        BGE        |   北京智源研究院   |              https://huggingface.co/BAAI/bge-base-en-v1.5              |
       - 对于一些罕见的专有名词，可以使用上述的模型进行微调来实现
       - 【注意】向量化也是通过模型来实现的
     - 数据入库：数据向量化后构建索引，并写入数据库的过程可以概述为数据入库过程，ES可以作为该种数据的数据库存储数据
   - 应用阶段：用户提问 -- 数据召回 -- 注入prompt -- LLM生成答案
     - 用户提问：用户向大模型提问
     - 数据召回：也就是数据检索，怎么找到跟当前问题最相关的答案？
       - 相似性检索：计算向量与所有存储向量的相似性得分，返回得分最高的记录，常见的相似性计算方法包括
         - 余弦相似性：计算两个向量之间的夹角大小 $$ \text{余弦相似性} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} $$
         - 欧式距离相似性：$$ d(\mathbf{A}, \mathbf{B}) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2} $$
         - 曼哈顿距离：$$ d(\mathbf{A}, \mathbf{B}) = \sum_{i=1}^{n} |A_i - B_i| $$
       - 全文检索：数据存入的时候构建倒排索引，检索的时候使用关键词定位记录的位置
     - 注入prompt：将知识注入prompt
     - LLM生成答案
     ```txt
     【任务】假设你是一个专业的客服机器人，请参考【背景知识】，回答【问题】
     【背景知识】1. a是A；2. b是B；3. c是C；4. d是D
     【问题】abcd是什么？
     ```
3. RAG的必要性
   1. 大模型不具备某一方面的专业能力，RAG作为一种外界的知识库对LLM的能力进行了提升
   2. 数据安全性考量：某些企业不愿意将数据传给第三方平台进行训练
   3. 幻觉问题：大模型具有幻觉，经常会胡说八道，大模型需要某一方面的系统性知识


## 二、RAG的多种类型
1. 原始RAG：数据召回 -- 注入prompt -- LLM生成答案
   ![原始RAG](../fig/RAG1.png)
2. 高级RAG：
   ![高级RAG](../fig/RAG2.png)
   1. 分块：是优化语义表示和计算效率的核心技术策略
      - 为什么要分块？Transformer的架构限制——当输入长度超过阈值，注意力机制的权重分布趋向于平均化，因此输入数据的长度需要限制
      - 分块有很多方式，有很多文本拆分器可以实现这个功能
      - 块的大小有很多选择方式
        - 法律文本：768 tokens大块（保持条款完整性）
        - 学术论文：512 tokens块 + 章节标题锚点
        - 社交媒体：64 tokens微型块 + 表情符号敏感分割
      - token的含义：在大模型语境中，token是语言模型处理文本的最小语义单元
        - 英语：平均每个token对应4字符（如"ing"作为后缀token） 
        - 中文：单个汉字通常占2-3个token（基于UTF-8的BPE编码），【汉字的含义多重化，用2-3个token来代表一个汉字】
        - 代码：特殊符号如"->"在PythonTokenizer中被映射为独立token
   2. 向量化


    


   





参考文档
1. 快速入门RAG整体流程：https://blog.csdn.net/m0_59164304/article/details/137628721
2. 