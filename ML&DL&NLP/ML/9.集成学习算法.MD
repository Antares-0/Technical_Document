# 集成学习算法

## 一、集成学习算法简介
1. 集成学习（Ensemble Learning）通过某种策略组合多个个体学习器的预测结果来提高整体的预测能力。
2. 分类：只包含同种类型的个体学习器的集成称为同质集成，例如决策树集成中全是决策树，同质集成中的个体学习器亦称基学习器，相应的学习算法称为基学习算法。包含不同类型的个体学习器的集成称为异质集成，例如同时包含决策树和神经网络。
3. 集成学习有三大经典方法：Boosting、Bagging、Stacking。
   - Boosting（提升方法）按顺序训练模型，每个模型关注前一个模型的错误，通过加权调整来优化整体预测。如AdaBoost通过给错分的样本更大的权重，逐步改进；梯度提升树用梯度下降法优化损失函数；XGBoost和LightGBM是高效的梯度提升树变种。Boosting主要关注于降低偏差。
   - Bagging（Bootstrap Aggregating，自助聚合）从原始数据集中通过有放回的对样本采样生成多个子数据集，分别训练多个独立模型，最后通过投票（分类）或平均（回归）得到结果。随机森林则是在Bagging基础上随机选择特征子集训练每棵树。Bagging主要关注于降低方差。
   - Stacking（堆叠）训练多个不同类型的个体学习器，之后使用一个元模型综合多个个体学习器的预测。灵活性强，能结合多种模型的优势。

## 二、工作原理
1. 集成学习的本质就是“三个臭皮匠胜于诸葛亮”，通过多个模块的堆叠，来实现更高的准确率
2. Boosting：写一个错题本，每次训练的时候重点关注上一个模型错误的数据
3. Bagging：用不同的数据训练不同的模型，最后通过投票和验证选一个最优的
4. Stacking：训练多个模型，最后整体使用

## <font color='yellow'>**三、关键点**</font>
1. Boosting的代表作——AdaBoost【对模型和数据给予更大的权重来实现优化】
   - 问题提出：在概率近似正确学习的框架中，一个概念如果存在一个多项式的学习算法能够学习它，并且正确率很高，就称这个概念是强可学习的；一个概念如果存在一个多项式的学习算法能够学习它，但正确率仅比随机猜测略好，就称这个概念是弱可学习的。后来证明，强可学习与弱可学习是等价的。那么如果已经发现了弱学习算法，能否通过某种方式将其提升为强学习算法？
   - 对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易的多。Boosting就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器构成一个强分类器。AdaBoost通常使用单层决策树作为基学习器，单层决策树也被称为决策树桩（Decision Stump）
   - 大多数Boosting都是改变训练数据的概率分布（权重分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。AdaBoost（Adaptive Boosting，自适应提升）的做法是提高被前一轮弱分类器错误分类的样本的权重，降低被正确分类的样本的权重。这样一来后一轮弱学习器会更加关注那些没有被正确分类的数据。同时采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。
   - 两手抓：针对预测正确的模型给予更大的权重，同时给予预测错误的数据更大的权重，来实现优化
2. Bagging的代表作——随机森林【对特征的选择和数据选择增加更多的随机性来实现优化】
   - 随机森林是Bagging的一个变体，在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树训练过程中引入了随机属性选择。
   - 具体来说，传统决策树在选择划分特征时是在当前节点的特征集合（假定有d个特征）中选择最优特征。而在随机森林中，基决策树的每个节点先从该节点的特征集合中随机选择一个包含`k`个特征的子集，然后再从这个子集中选择一个最优特征用于划分。参数`k`控制着随机性的引入程度，若`k=d`，则基决策树的生成与传统决策树相同；若`k=1`，则随机选择一个属性用于划分。一般推荐$k = \log_2 d$
   - 随机森林简单易实现，但在很多任务中都展现出了强大性能，被誉为“代表集成学习技术水平的方法”。Bagging中基学习器的多样性仅来自于样本扰动，而随机森林中基学习器的多样性不仅来自样本扰动，还来自特征扰动，这就使得最终集成的泛化性能可通过基学习器之间差异度的增加而进一步提升

## 四、实战

参考资料：
1. 尚硅谷机器学习视频：https://www.bilibili.com/video/BV1BYe4z5E9z

