# SVM算法

## 一、SVM算法简介
1. 支持向量机（Support Vector Machines，SVM）是一种二分类模型。其核心目标是寻找一个“间隔最大”的超平面将不同类别的数据点分隔开。
2. 间隔最大代表该超平面与最近的数据点之间的距离最大，这使得支持向量机有较强的泛化能力。支持向量机学习方法包括由简至繁的模型：线性可分支持向量机、线性支持向量机以及非线性支持向量机
3. 是一种分类模型，不适用于回归

## <font color='yellow'>**二、关键点**</font>
1. SVM分为三种，分别是
   - 线性可分支持向量机——硬间隔
   - 线性支持向量机——软间隔
   - 非线性支持向量机——核函数
2. 线性可分支持向量机——硬间隔
   - 硬间隔是指超平面能够将不同类的样本完全划分开，能够完全分开，“数据比较好”
   - 数据是线性可分的：可以用一条线直接完美划分数据
   - 距离超平面最近的几个样本点称为支持向量，它们直接决定超平面的位置和方向，只要支持向量不变，超平面就不会变
   - 间隔：两个异类支持向量到超平面的距离之和
   - 最大间隔：两个异类支持向量到超平面的距离之和 取最大时的 间隔
   - 计算方法由数学给出，可以求解出具有最大间隔的超平面
3. 线性支持向量机——软间隔
   - 软间隔：通常训练数据中会有一些特异点，如果将这些特异点去掉，剩下大部分样本点是线性可分的。这时我们可以放宽条件，允许某些样本分错，为此我们引入软间隔
   - 简单说就是数据没有那么完美，除了个别的点，其余的能够正常分类
   - 数据是线性不可分的，不可以用一条线直接完美划分数据
   - 引入了松弛变量和惩罚机制，间接使用SVM解决了分类问题
4. 非线性支持向量机——核函数
   - 非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题
   - 引入核函数来解决问题：可以通过核函数将数据从原始空间映射到高维特征空间，使得数据在高维特征空间线性可分，将原本的非线性问题转换为线性问题。使用核技巧学习非线性支持向量机，等价于隐式地在高维特征空间中学习线性支持向量机
   - 相当于将原来的数据引入一个额外的维度，然后在额外的维度上实现支持向量机
   - 核函数的选择也是支持向量机最大的变数，若核函数选择不合适，意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳

     |   核函数类型    |                                   公式                                    |
     |:----------:|:-----------------------------------------------------------------------:|
     |    线性核     |                      $\kappa(x_i,x_j) = x_i^T x_j$                      |
     |    多项式核    |                    $\kappa(x_i,x_j) = (x_i^T x_j)^d$                    |
     |    高斯核     |  $\kappa(x_i,x_j) = \exp\left(-\frac{\|x_i-x_j\|^2}{2\sigma^2}\right)$  |
     |   拉普拉斯核    |    $\kappa(x_i,x_j) = \exp\left(-\frac{\|x_i-x_j\|}{\sigma}\right)$     |
     |  Sigmoid核  |            $\kappa(x_i,x_j) = \tanh(\beta x_i^T x_j+\theta)$            |

## 三、优缺点
1. 支持向量机在某一段时间内成为了分类问题的主流解决方案，但是近些年使用神经网络的方法已经获得了更好的效果

## 四、实战

参考资料：
1. 尚硅谷机器学习视频：https://www.bilibili.com/video/BV1BYe4z5E9z

